{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "import scipy.io as sio\n",
    "\n",
    "from vehicle_model_variant_3 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driving_cycle_path = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "driving_cycle = sio.loadmat(driving_cycle_path)\n",
    "driving_cycle = driving_cycle[\"sch_cycle\"][:, 1]\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_nimh_6_240_panasonic_MY01_Prius.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_pm_95_145_X2.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, driving_cycle, battery_path, motor_path, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATE_SIZE = env.calculation_comp[\"state_size\"]\n",
    "STATE_SIZE = 4\n",
    "ACTION_SIZE = env.calculation_comp[\"action_size\"] \n",
    "LEARNING_RATE = 0.00025 \n",
    "\n",
    "TOTAL_EPISODES = 200\n",
    "MAX_STEPS = 50000 \n",
    "\n",
    "GAMMA = 0.95 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "TAU = 0.001 \n",
    "DELAY_TRAINING = 3000 \n",
    "EPSILON_MIN_ITER = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "target_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "\n",
    "primary_network.compile(\n",
    "    loss=\"mse\", \n",
    "    optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    ")\n",
    "\n",
    "# for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "#     t.assign(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network(primary_network, target_network): \n",
    "    for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "        t.assign(t * (1 - TAU) + p * TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory: \n",
    "    def __init__(self, max_memory): \n",
    "        self.max_memory = max_memory \n",
    "        self._samples = [] \n",
    "        \n",
    "    def add_sample(self, sample): \n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self.max_memory: \n",
    "            self._samples.pop(0)\n",
    "        \n",
    "    def sample(self, no_samples): \n",
    "        if no_samples > len(self._samples): \n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else: \n",
    "            return random.sample(self._samples, no_samples)\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)\n",
    "    \n",
    "\n",
    "# memory = Memory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, primary_network, eps):\n",
    "    j_min = state[-2]\n",
    "    j_max = state[-1]\n",
    "    \n",
    "    if random.random() < eps:\n",
    "        action = random.randint(0, ACTION_SIZE - 1) \n",
    "        action_continue = np.linspace(j_min, j_max, ACTION_SIZE)[action]\n",
    "        return action, action_continue \n",
    "    else: \n",
    "        action = np.argmax(primary_network(np.array(state).reshape(1, -1)))\n",
    "        action_continue = np.linspace(j_min, j_max, ACTION_SIZE)[action]\n",
    "        return action, action_continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(primary_network, target_network, memory): \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    states = np.array([val[0] for val in batch]) \n",
    "    actions = np.array([val[1] for val in batch])\n",
    "    rewards = np.array([val[2] for val in batch])\n",
    "    next_states = np.array([np.zeros(STATE_SIZE) if val[3] is None else val[3]  \n",
    "                            for val in batch])\n",
    "    \n",
    "    prim_qt = primary_network(states)\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "    target_q = prim_qt.numpy() \n",
    "    updates = rewards \n",
    "    valid_idxs = next_states.sum(axis=1) != 0 \n",
    "    batch_idxs = np.arange(BATCH_SIZE)\n",
    "    prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "    q_from_target = target_network(next_states)\n",
    "    updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], \n",
    "                                                        prim_action_tp1[valid_idxs]]\n",
    "    \n",
    "    target_q[batch_idxs, actions] = updates \n",
    "    loss = primary_network.train_on_batch(states, target_q)\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_with_rewardFactor(reward_factor):\n",
    "    env = Environment(cell_model, driving_cycle, battery_path, motor_path, reward_factor)\n",
    "    \n",
    "    memory = Memory(10000)\n",
    "    \n",
    "    primary_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(),  \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    target_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    primary_network.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    "    )\n",
    "    return env, memory, primary_network, target_network \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment version: 1\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 0\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 1\n",
      "WARNING:tensorflow:Layer sequential_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer sequential_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 3 Total reward: -4996.201093826403 Explore P: 0.9217 SOC: 1.0000 Cumulative_SOC_deviation: 480.3159 Fuel Consumption: 193.0425\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 4 Total reward: -5010.171288578992 Explore P: 0.8970 SOC: 1.0000 Cumulative_SOC_deviation: 482.6015 Fuel Consumption: 184.1560\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 5 Total reward: -4958.354569294941 Explore P: 0.8730 SOC: 1.0000 Cumulative_SOC_deviation: 477.2685 Fuel Consumption: 185.6697\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 6 Total reward: -4865.105863539814 Explore P: 0.8496 SOC: 1.0000 Cumulative_SOC_deviation: 468.9807 Fuel Consumption: 175.2988\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 7 Total reward: -4773.042898392224 Explore P: 0.8269 SOC: 0.9989 Cumulative_SOC_deviation: 459.7256 Fuel Consumption: 175.7871\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 8 Total reward: -4823.972341080481 Explore P: 0.8048 SOC: 0.9997 Cumulative_SOC_deviation: 464.8581 Fuel Consumption: 175.3916\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 9 Total reward: -4853.744039977627 Explore P: 0.7832 SOC: 1.0000 Cumulative_SOC_deviation: 467.3943 Fuel Consumption: 179.8007\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 10 Total reward: -4549.487182890679 Explore P: 0.7623 SOC: 0.9998 Cumulative_SOC_deviation: 437.5155 Fuel Consumption: 174.3320\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 11 Total reward: -4415.303052696679 Explore P: 0.7419 SOC: 1.0000 Cumulative_SOC_deviation: 423.9467 Fuel Consumption: 175.8359\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 12 Total reward: -4251.520175015027 Explore P: 0.7221 SOC: 1.0000 Cumulative_SOC_deviation: 408.5269 Fuel Consumption: 166.2512\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 13 Total reward: -4002.814885675035 Explore P: 0.7028 SOC: 1.0000 Cumulative_SOC_deviation: 383.4210 Fuel Consumption: 168.6046\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 14 Total reward: -4119.1952843027975 Explore P: 0.6840 SOC: 0.9995 Cumulative_SOC_deviation: 395.5610 Fuel Consumption: 163.5852\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 15 Total reward: -3780.789960067313 Explore P: 0.6658 SOC: 0.9977 Cumulative_SOC_deviation: 362.1121 Fuel Consumption: 159.6693\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 16 Total reward: -3248.4291874764967 Explore P: 0.6480 SOC: 0.9999 Cumulative_SOC_deviation: 308.6699 Fuel Consumption: 161.7298\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 17 Total reward: -3392.8954444434307 Explore P: 0.6307 SOC: 1.0000 Cumulative_SOC_deviation: 322.5790 Fuel Consumption: 167.1056\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 18 Total reward: -3522.7105483996506 Explore P: 0.6139 SOC: 0.9993 Cumulative_SOC_deviation: 335.8847 Fuel Consumption: 163.8635\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 19 Total reward: -3711.3316203182126 Explore P: 0.5976 SOC: 0.9993 Cumulative_SOC_deviation: 355.7160 Fuel Consumption: 154.1714\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 20 Total reward: -2823.548011525701 Explore P: 0.5816 SOC: 0.9974 Cumulative_SOC_deviation: 266.5134 Fuel Consumption: 158.4144\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 21 Total reward: -2401.1359545570335 Explore P: 0.5662 SOC: 0.9884 Cumulative_SOC_deviation: 224.8708 Fuel Consumption: 152.4282\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 22 Total reward: -2290.626791617408 Explore P: 0.5511 SOC: 0.9999 Cumulative_SOC_deviation: 213.3213 Fuel Consumption: 157.4135\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 23 Total reward: -2509.8061577104013 Explore P: 0.5364 SOC: 0.9916 Cumulative_SOC_deviation: 235.7656 Fuel Consumption: 152.1499\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 24 Total reward: -2322.362562144189 Explore P: 0.5222 SOC: 0.9704 Cumulative_SOC_deviation: 217.3093 Fuel Consumption: 149.2691\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 25 Total reward: -1939.7103320939023 Explore P: 0.5083 SOC: 0.9197 Cumulative_SOC_deviation: 179.2966 Fuel Consumption: 146.7448\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 26 Total reward: -1577.6012183771977 Explore P: 0.4948 SOC: 0.8957 Cumulative_SOC_deviation: 143.1838 Fuel Consumption: 145.7633\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 27 Total reward: -1479.1297840669495 Explore P: 0.4817 SOC: 0.8912 Cumulative_SOC_deviation: 133.3220 Fuel Consumption: 145.9098\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 28 Total reward: -1018.9340693108551 Explore P: 0.4689 SOC: 0.7794 Cumulative_SOC_deviation: 88.1452 Fuel Consumption: 137.4823\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 29 Total reward: -410.68349582165007 Explore P: 0.4565 SOC: 0.6480 Cumulative_SOC_deviation: 28.2874 Fuel Consumption: 127.8097\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 30 Total reward: -750.674937964332 Explore P: 0.4444 SOC: 0.7778 Cumulative_SOC_deviation: 61.4281 Fuel Consumption: 136.3934\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 31 Total reward: -1183.6369885095396 Explore P: 0.4326 SOC: 1.0000 Cumulative_SOC_deviation: 102.7854 Fuel Consumption: 155.7827\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 32 Total reward: -511.9594648886518 Explore P: 0.4212 SOC: 0.6422 Cumulative_SOC_deviation: 38.4736 Fuel Consumption: 127.2237\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 33 Total reward: -491.6643326496397 Explore P: 0.4100 SOC: 0.5788 Cumulative_SOC_deviation: 36.9069 Fuel Consumption: 122.5949\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 34 Total reward: -558.2416818589494 Explore P: 0.3992 SOC: 0.6810 Cumulative_SOC_deviation: 42.6780 Fuel Consumption: 131.4619\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 35 Total reward: -607.9555175098128 Explore P: 0.3887 SOC: 0.7166 Cumulative_SOC_deviation: 47.6625 Fuel Consumption: 131.3301\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 36 Total reward: -621.9872333453985 Explore P: 0.3784 SOC: 0.6495 Cumulative_SOC_deviation: 49.5144 Fuel Consumption: 126.8429\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 37 Total reward: -894.2712415485679 Explore P: 0.3684 SOC: 0.6268 Cumulative_SOC_deviation: 76.7560 Fuel Consumption: 126.7111\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 38 Total reward: -580.2122493396628 Explore P: 0.3587 SOC: 0.6368 Cumulative_SOC_deviation: 45.5479 Fuel Consumption: 124.7336\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 39 Total reward: -666.9620722665219 Explore P: 0.3493 SOC: 0.6397 Cumulative_SOC_deviation: 54.4099 Fuel Consumption: 122.8635\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 40 Total reward: -4570.922177875927 Explore P: 0.3401 SOC: 0.9948 Cumulative_SOC_deviation: 440.5858 Fuel Consumption: 165.0647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 41 Total reward: -422.5068446418218 Explore P: 0.3311 SOC: 0.6279 Cumulative_SOC_deviation: 30.2690 Fuel Consumption: 119.8167\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 42 Total reward: -427.20437744931417 Explore P: 0.3224 SOC: 0.6217 Cumulative_SOC_deviation: 30.6699 Fuel Consumption: 120.5051\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 43 Total reward: -504.4543976955268 Explore P: 0.3140 SOC: 0.6406 Cumulative_SOC_deviation: 38.1254 Fuel Consumption: 123.2004\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 44 Total reward: -459.75505438541217 Explore P: 0.3057 SOC: 0.6366 Cumulative_SOC_deviation: 33.8942 Fuel Consumption: 120.8127\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 45 Total reward: -345.62562778426343 Explore P: 0.2977 SOC: 0.6168 Cumulative_SOC_deviation: 22.5780 Fuel Consumption: 119.8460\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 46 Total reward: -393.9231918808678 Explore P: 0.2899 SOC: 0.5820 Cumulative_SOC_deviation: 27.5811 Fuel Consumption: 118.1126\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 47 Total reward: -374.0558678861146 Explore P: 0.2824 SOC: 0.6296 Cumulative_SOC_deviation: 25.2345 Fuel Consumption: 121.7112\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 48 Total reward: -368.70484274571766 Explore P: 0.2750 SOC: 0.6190 Cumulative_SOC_deviation: 24.7633 Fuel Consumption: 121.0715\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 49 Total reward: -448.98984906691 Explore P: 0.2678 SOC: 0.6458 Cumulative_SOC_deviation: 32.6507 Fuel Consumption: 122.4826\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 50 Total reward: -403.45173442869293 Explore P: 0.2608 SOC: 0.5851 Cumulative_SOC_deviation: 28.5837 Fuel Consumption: 117.6146\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 51 Total reward: -388.4782494689953 Explore P: 0.2540 SOC: 0.5874 Cumulative_SOC_deviation: 27.0248 Fuel Consumption: 118.2298\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 52 Total reward: -396.1540453821609 Explore P: 0.2474 SOC: 0.5998 Cumulative_SOC_deviation: 27.7104 Fuel Consumption: 119.0501\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 53 Total reward: -369.63621820298596 Explore P: 0.2410 SOC: 0.6027 Cumulative_SOC_deviation: 25.0508 Fuel Consumption: 119.1282\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 54 Total reward: -374.8412935472947 Explore P: 0.2347 SOC: 0.6594 Cumulative_SOC_deviation: 25.4229 Fuel Consumption: 120.6126\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 55 Total reward: -309.4920191028323 Explore P: 0.2286 SOC: 0.6154 Cumulative_SOC_deviation: 19.2126 Fuel Consumption: 117.3656\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 56 Total reward: -284.4368361616267 Explore P: 0.2227 SOC: 0.6132 Cumulative_SOC_deviation: 16.6681 Fuel Consumption: 117.7562\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 57 Total reward: -243.96416476678291 Explore P: 0.2170 SOC: 0.6091 Cumulative_SOC_deviation: 12.8029 Fuel Consumption: 115.9349\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 58 Total reward: -221.2760981346418 Explore P: 0.2114 SOC: 0.6105 Cumulative_SOC_deviation: 10.4731 Fuel Consumption: 116.5453\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 59 Total reward: -211.26646250808335 Explore P: 0.2059 SOC: 0.6202 Cumulative_SOC_deviation: 9.4292 Fuel Consumption: 116.9749\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 60 Total reward: -220.8399875108705 Explore P: 0.2006 SOC: 0.6028 Cumulative_SOC_deviation: 10.4300 Fuel Consumption: 116.5404\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 61 Total reward: -200.14209334271945 Explore P: 0.1954 SOC: 0.6138 Cumulative_SOC_deviation: 8.3817 Fuel Consumption: 116.3255\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 62 Total reward: -177.46283078356564 Explore P: 0.1904 SOC: 0.6011 Cumulative_SOC_deviation: 6.3247 Fuel Consumption: 114.2162\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 63 Total reward: -170.06975481919625 Explore P: 0.1855 SOC: 0.6034 Cumulative_SOC_deviation: 5.6542 Fuel Consumption: 113.5278\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 64 Total reward: -171.68307432267386 Explore P: 0.1808 SOC: 0.6016 Cumulative_SOC_deviation: 5.7257 Fuel Consumption: 114.4262\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 65 Total reward: -160.38222665213968 Explore P: 0.1761 SOC: 0.6016 Cumulative_SOC_deviation: 4.5727 Fuel Consumption: 114.6557\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 66 Total reward: -158.22011417244687 Explore P: 0.1716 SOC: 0.6035 Cumulative_SOC_deviation: 4.4526 Fuel Consumption: 113.6938\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 67 Total reward: -162.46738008452897 Explore P: 0.1673 SOC: 0.6029 Cumulative_SOC_deviation: 4.8520 Fuel Consumption: 113.9477\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 68 Total reward: -159.7304391502424 Explore P: 0.1630 SOC: 0.6013 Cumulative_SOC_deviation: 4.5407 Fuel Consumption: 114.3236\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 69 Total reward: -154.6709393977841 Explore P: 0.1589 SOC: 0.5989 Cumulative_SOC_deviation: 4.0616 Fuel Consumption: 114.0551\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 70 Total reward: -159.1630789917663 Explore P: 0.1548 SOC: 0.5981 Cumulative_SOC_deviation: 4.4478 Fuel Consumption: 114.6850\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 71 Total reward: -159.89070001352405 Explore P: 0.1509 SOC: 0.5989 Cumulative_SOC_deviation: 4.5245 Fuel Consumption: 114.6459\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 72 Total reward: -154.41883945230308 Explore P: 0.1471 SOC: 0.5997 Cumulative_SOC_deviation: 4.0798 Fuel Consumption: 113.6205\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 73 Total reward: -2409.6170400546744 Explore P: 0.1434 SOC: 0.9937 Cumulative_SOC_deviation: 226.2828 Fuel Consumption: 146.7887\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 74 Total reward: -153.74955920043806 Explore P: 0.1398 SOC: 0.6023 Cumulative_SOC_deviation: 4.0749 Fuel Consumption: 113.0004\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 75 Total reward: -152.0605391186831 Explore P: 0.1362 SOC: 0.6039 Cumulative_SOC_deviation: 3.9290 Fuel Consumption: 112.7709\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 76 Total reward: -154.43490355175084 Explore P: 0.1328 SOC: 0.6006 Cumulative_SOC_deviation: 4.1937 Fuel Consumption: 112.4975\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 77 Total reward: -155.3804515377909 Explore P: 0.1295 SOC: 0.6048 Cumulative_SOC_deviation: 4.2566 Fuel Consumption: 112.8149\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 78 Total reward: -162.99582636269014 Explore P: 0.1263 SOC: 0.6034 Cumulative_SOC_deviation: 4.9571 Fuel Consumption: 113.4252\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 79 Total reward: -151.65810439572334 Explore P: 0.1231 SOC: 0.6072 Cumulative_SOC_deviation: 3.9478 Fuel Consumption: 112.1801\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 80 Total reward: -157.32142035817634 Explore P: 0.1200 SOC: 0.5980 Cumulative_SOC_deviation: 4.5405 Fuel Consumption: 111.9165\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 81 Total reward: -164.7856953594237 Explore P: 0.1171 SOC: 0.6035 Cumulative_SOC_deviation: 5.2713 Fuel Consumption: 112.0727\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 82 Total reward: -162.97118418023751 Explore P: 0.1142 SOC: 0.5983 Cumulative_SOC_deviation: 5.1284 Fuel Consumption: 111.6870\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 83 Total reward: -209.02113065006165 Explore P: 0.1113 SOC: 0.5980 Cumulative_SOC_deviation: 9.5733 Fuel Consumption: 113.2885\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 84 Total reward: -166.8517785302663 Explore P: 0.1086 SOC: 0.6049 Cumulative_SOC_deviation: 5.4491 Fuel Consumption: 112.3608\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 85 Total reward: -163.99656609715487 Explore P: 0.1059 SOC: 0.6028 Cumulative_SOC_deviation: 5.0962 Fuel Consumption: 113.0346\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 86 Total reward: -153.15290212542774 Explore P: 0.1033 SOC: 0.6003 Cumulative_SOC_deviation: 3.9493 Fuel Consumption: 113.6596\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 87 Total reward: -150.83246998334207 Explore P: 0.1008 SOC: 0.5999 Cumulative_SOC_deviation: 3.7554 Fuel Consumption: 113.2787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 88 Total reward: -143.33620533105267 Explore P: 0.0983 SOC: 0.6003 Cumulative_SOC_deviation: 2.9808 Fuel Consumption: 113.5278\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 89 Total reward: -148.45962986164335 Explore P: 0.0960 SOC: 0.6011 Cumulative_SOC_deviation: 3.5879 Fuel Consumption: 112.5805\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 90 Total reward: -154.05187100408847 Explore P: 0.0936 SOC: 0.6106 Cumulative_SOC_deviation: 4.0016 Fuel Consumption: 114.0356\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 91 Total reward: -143.13789831704852 Explore P: 0.0914 SOC: 0.5993 Cumulative_SOC_deviation: 3.0396 Fuel Consumption: 112.7416\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 92 Total reward: -146.7868932349826 Explore P: 0.0892 SOC: 0.6011 Cumulative_SOC_deviation: 3.3821 Fuel Consumption: 112.9662\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 93 Total reward: -155.1551163158561 Explore P: 0.0870 SOC: 0.6012 Cumulative_SOC_deviation: 4.1925 Fuel Consumption: 113.2299\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 94 Total reward: -144.17781683196213 Explore P: 0.0849 SOC: 0.6003 Cumulative_SOC_deviation: 3.1627 Fuel Consumption: 112.5512\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 95 Total reward: -147.0745560651723 Explore P: 0.0829 SOC: 0.6020 Cumulative_SOC_deviation: 3.3718 Fuel Consumption: 113.3569\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 96 Total reward: -142.12553433847424 Explore P: 0.0809 SOC: 0.5983 Cumulative_SOC_deviation: 2.9892 Fuel Consumption: 112.2338\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 97 Total reward: -149.4756260909625 Explore P: 0.0790 SOC: 0.5987 Cumulative_SOC_deviation: 3.8443 Fuel Consumption: 111.0327\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 98 Total reward: -144.25121391574302 Explore P: 0.0771 SOC: 0.6013 Cumulative_SOC_deviation: 3.2091 Fuel Consumption: 112.1606\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 99 Total reward: -146.10864850874106 Explore P: 0.0753 SOC: 0.6001 Cumulative_SOC_deviation: 3.2703 Fuel Consumption: 113.4057\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 100 Total reward: -150.21354644708921 Explore P: 0.0735 SOC: 0.6018 Cumulative_SOC_deviation: 3.6959 Fuel Consumption: 113.2543\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 101 Total reward: -142.81482490100134 Explore P: 0.0718 SOC: 0.5992 Cumulative_SOC_deviation: 3.0337 Fuel Consumption: 112.4780\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 102 Total reward: -139.58920239022615 Explore P: 0.0701 SOC: 0.6044 Cumulative_SOC_deviation: 2.6437 Fuel Consumption: 113.1518\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 103 Total reward: -139.6922922873518 Explore P: 0.0685 SOC: 0.6009 Cumulative_SOC_deviation: 2.7546 Fuel Consumption: 112.1460\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 104 Total reward: -138.9040418864607 Explore P: 0.0669 SOC: 0.6004 Cumulative_SOC_deviation: 2.7915 Fuel Consumption: 110.9888\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 105 Total reward: -142.20883193327023 Explore P: 0.0654 SOC: 0.6027 Cumulative_SOC_deviation: 2.9975 Fuel Consumption: 112.2338\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 106 Total reward: -139.83538028240045 Explore P: 0.0639 SOC: 0.5996 Cumulative_SOC_deviation: 2.7343 Fuel Consumption: 112.4926\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 107 Total reward: -136.24185757803585 Explore P: 0.0624 SOC: 0.5995 Cumulative_SOC_deviation: 2.4565 Fuel Consumption: 111.6772\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 108 Total reward: -140.09185715205302 Explore P: 0.0610 SOC: 0.5998 Cumulative_SOC_deviation: 2.8410 Fuel Consumption: 111.6821\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 109 Total reward: -136.07157836880245 Explore P: 0.0596 SOC: 0.5996 Cumulative_SOC_deviation: 2.5864 Fuel Consumption: 110.2075\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 110 Total reward: -135.63342825469334 Explore P: 0.0583 SOC: 0.6033 Cumulative_SOC_deviation: 2.4176 Fuel Consumption: 111.4575\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 111 Total reward: -135.05780710108613 Explore P: 0.0570 SOC: 0.5990 Cumulative_SOC_deviation: 2.4367 Fuel Consumption: 110.6909\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 112 Total reward: -139.07994854345296 Explore P: 0.0557 SOC: 0.5994 Cumulative_SOC_deviation: 2.7740 Fuel Consumption: 111.3403\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 113 Total reward: -145.47202568674928 Explore P: 0.0545 SOC: 0.5995 Cumulative_SOC_deviation: 3.3834 Fuel Consumption: 111.6382\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 114 Total reward: -137.8899226472731 Explore P: 0.0533 SOC: 0.6008 Cumulative_SOC_deviation: 2.6462 Fuel Consumption: 111.4282\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 115 Total reward: -142.4840950176449 Explore P: 0.0521 SOC: 0.5996 Cumulative_SOC_deviation: 3.1344 Fuel Consumption: 111.1401\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 116 Total reward: -138.55888482607546 Explore P: 0.0510 SOC: 0.6015 Cumulative_SOC_deviation: 2.6999 Fuel Consumption: 111.5600\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 117 Total reward: -140.38835116627723 Explore P: 0.0498 SOC: 0.5989 Cumulative_SOC_deviation: 2.9448 Fuel Consumption: 110.9399\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 118 Total reward: -144.09702575142762 Explore P: 0.0488 SOC: 0.5980 Cumulative_SOC_deviation: 3.2620 Fuel Consumption: 111.4770\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 119 Total reward: -147.02772173057454 Explore P: 0.0477 SOC: 0.6006 Cumulative_SOC_deviation: 3.5473 Fuel Consumption: 111.5551\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 120 Total reward: -152.77448443220217 Explore P: 0.0467 SOC: 0.5990 Cumulative_SOC_deviation: 3.9628 Fuel Consumption: 113.1469\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 121 Total reward: -149.01242254814153 Explore P: 0.0457 SOC: 0.5985 Cumulative_SOC_deviation: 3.7457 Fuel Consumption: 111.5551\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 122 Total reward: -147.26790200500656 Explore P: 0.0447 SOC: 0.5985 Cumulative_SOC_deviation: 3.6626 Fuel Consumption: 110.6421\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 123 Total reward: -145.45129769687773 Explore P: 0.0438 SOC: 0.5983 Cumulative_SOC_deviation: 3.4131 Fuel Consumption: 111.3208\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 124 Total reward: -146.3519814941743 Explore P: 0.0429 SOC: 0.5984 Cumulative_SOC_deviation: 3.4694 Fuel Consumption: 111.6577\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 125 Total reward: -136.8490872889363 Explore P: 0.0420 SOC: 0.5993 Cumulative_SOC_deviation: 2.5050 Fuel Consumption: 111.7993\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 126 Total reward: -139.54613830323575 Explore P: 0.0411 SOC: 0.5981 Cumulative_SOC_deviation: 2.6736 Fuel Consumption: 112.8100\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 127 Total reward: -143.90384763359924 Explore P: 0.0403 SOC: 0.5991 Cumulative_SOC_deviation: 3.0820 Fuel Consumption: 113.0834\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 128 Total reward: -140.77541944763067 Explore P: 0.0395 SOC: 0.5985 Cumulative_SOC_deviation: 3.0514 Fuel Consumption: 110.2612\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 129 Total reward: -148.46648578228226 Explore P: 0.0387 SOC: 0.5978 Cumulative_SOC_deviation: 3.6653 Fuel Consumption: 111.8139\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 130 Total reward: -152.74257536785368 Explore P: 0.0379 SOC: 0.5984 Cumulative_SOC_deviation: 4.0914 Fuel Consumption: 111.8286\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 131 Total reward: -3344.711066289652 Explore P: 0.0371 SOC: 1.0000 Cumulative_SOC_deviation: 314.7299 Fuel Consumption: 197.4126\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 132 Total reward: -152.2872612625836 Explore P: 0.0364 SOC: 0.6007 Cumulative_SOC_deviation: 3.9629 Fuel Consumption: 112.6586\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 133 Total reward: -142.06259780572313 Explore P: 0.0357 SOC: 0.5983 Cumulative_SOC_deviation: 3.0713 Fuel Consumption: 111.3501\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 134 Total reward: -145.04615895986163 Explore P: 0.0350 SOC: 0.6007 Cumulative_SOC_deviation: 3.4970 Fuel Consumption: 110.0757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 135 Total reward: -145.27661840534577 Explore P: 0.0343 SOC: 0.5961 Cumulative_SOC_deviation: 3.4224 Fuel Consumption: 111.0522\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 136 Total reward: -142.6905116521552 Explore P: 0.0336 SOC: 0.5972 Cumulative_SOC_deviation: 3.1072 Fuel Consumption: 111.6186\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 137 Total reward: -144.77843275470374 Explore P: 0.0330 SOC: 0.5995 Cumulative_SOC_deviation: 3.2383 Fuel Consumption: 112.3950\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 138 Total reward: -152.4032599364592 Explore P: 0.0324 SOC: 0.5982 Cumulative_SOC_deviation: 4.1253 Fuel Consumption: 111.1499\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 139 Total reward: -148.9239039165399 Explore P: 0.0318 SOC: 0.5975 Cumulative_SOC_deviation: 3.6700 Fuel Consumption: 112.2241\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 140 Total reward: -146.8696822641662 Explore P: 0.0312 SOC: 0.6009 Cumulative_SOC_deviation: 3.5280 Fuel Consumption: 111.5893\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 141 Total reward: -153.07240855482274 Explore P: 0.0306 SOC: 0.5981 Cumulative_SOC_deviation: 3.9847 Fuel Consumption: 113.2250\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 142 Total reward: -152.58791798711752 Explore P: 0.0301 SOC: 0.5991 Cumulative_SOC_deviation: 3.9949 Fuel Consumption: 112.6391\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 143 Total reward: -156.8268263739438 Explore P: 0.0295 SOC: 0.5976 Cumulative_SOC_deviation: 4.3841 Fuel Consumption: 112.9858\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 144 Total reward: -155.36174636680454 Explore P: 0.0290 SOC: 0.5957 Cumulative_SOC_deviation: 4.2054 Fuel Consumption: 113.3080\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 145 Total reward: -158.6423870351618 Explore P: 0.0285 SOC: 0.5981 Cumulative_SOC_deviation: 4.5686 Fuel Consumption: 112.9565\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 146 Total reward: -159.50424014313901 Explore P: 0.0280 SOC: 0.6004 Cumulative_SOC_deviation: 4.7143 Fuel Consumption: 112.3608\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 147 Total reward: -160.1522516739644 Explore P: 0.0275 SOC: 0.5952 Cumulative_SOC_deviation: 4.8739 Fuel Consumption: 111.4136\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 148 Total reward: -160.181558705431 Explore P: 0.0270 SOC: 0.5972 Cumulative_SOC_deviation: 4.8695 Fuel Consumption: 111.4868\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 149 Total reward: -166.3628754461405 Explore P: 0.0265 SOC: 0.5971 Cumulative_SOC_deviation: 5.5135 Fuel Consumption: 111.2280\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 150 Total reward: -171.36153828353844 Explore P: 0.0261 SOC: 0.6010 Cumulative_SOC_deviation: 5.8010 Fuel Consumption: 113.3520\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 151 Total reward: -167.09646850622303 Explore P: 0.0257 SOC: 0.5937 Cumulative_SOC_deviation: 5.5424 Fuel Consumption: 111.6723\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 152 Total reward: -171.30864950550742 Explore P: 0.0252 SOC: 0.5943 Cumulative_SOC_deviation: 6.0588 Fuel Consumption: 110.7202\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 153 Total reward: -166.45693570424868 Explore P: 0.0248 SOC: 0.5948 Cumulative_SOC_deviation: 5.2973 Fuel Consumption: 113.4838\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 154 Total reward: -168.2491984900343 Explore P: 0.0244 SOC: 0.5975 Cumulative_SOC_deviation: 5.6342 Fuel Consumption: 111.9067\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 155 Total reward: -168.8762063925155 Explore P: 0.0240 SOC: 0.5920 Cumulative_SOC_deviation: 5.7155 Fuel Consumption: 111.7212\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 156 Total reward: -167.06475234845894 Explore P: 0.0237 SOC: 0.5958 Cumulative_SOC_deviation: 5.5798 Fuel Consumption: 111.2671\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 157 Total reward: -177.52215276157483 Explore P: 0.0233 SOC: 0.5969 Cumulative_SOC_deviation: 6.5957 Fuel Consumption: 111.5649\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 158 Total reward: -155.41444746388 Explore P: 0.0229 SOC: 0.5953 Cumulative_SOC_deviation: 4.4084 Fuel Consumption: 111.3305\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 159 Total reward: -165.90110159960264 Explore P: 0.0226 SOC: 0.5996 Cumulative_SOC_deviation: 5.3155 Fuel Consumption: 112.7465\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 160 Total reward: -152.82222625373794 Explore P: 0.0222 SOC: 0.6002 Cumulative_SOC_deviation: 3.9744 Fuel Consumption: 113.0786\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 161 Total reward: -4785.707705337521 Explore P: 0.0219 SOC: 1.0000 Cumulative_SOC_deviation: 454.2422 Fuel Consumption: 243.2855\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 162 Total reward: -185.49919284832362 Explore P: 0.0216 SOC: 0.5962 Cumulative_SOC_deviation: 7.1913 Fuel Consumption: 113.5864\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 163 Total reward: -160.35512117985118 Explore P: 0.0213 SOC: 0.5971 Cumulative_SOC_deviation: 4.9728 Fuel Consumption: 110.6274\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 164 Total reward: -153.35892528633067 Explore P: 0.0210 SOC: 0.5981 Cumulative_SOC_deviation: 4.2160 Fuel Consumption: 111.1987\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 165 Total reward: -159.9244789418711 Explore P: 0.0207 SOC: 0.6061 Cumulative_SOC_deviation: 4.7954 Fuel Consumption: 111.9702\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 166 Total reward: -157.11509064350383 Explore P: 0.0204 SOC: 0.5971 Cumulative_SOC_deviation: 4.5067 Fuel Consumption: 112.0483\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 167 Total reward: -153.16203583128367 Explore P: 0.0201 SOC: 0.5951 Cumulative_SOC_deviation: 4.2359 Fuel Consumption: 110.8032\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 168 Total reward: -150.9293448633273 Explore P: 0.0198 SOC: 0.5984 Cumulative_SOC_deviation: 3.9457 Fuel Consumption: 111.4721\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 169 Total reward: -157.03339572930608 Explore P: 0.0196 SOC: 0.5939 Cumulative_SOC_deviation: 4.7383 Fuel Consumption: 109.6509\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 170 Total reward: -156.68211124872877 Explore P: 0.0193 SOC: 0.5958 Cumulative_SOC_deviation: 4.6714 Fuel Consumption: 109.9683\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 171 Total reward: -171.07010782730933 Explore P: 0.0190 SOC: 0.5971 Cumulative_SOC_deviation: 5.9574 Fuel Consumption: 111.4966\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 172 Total reward: -155.05465838810034 Explore P: 0.0188 SOC: 0.5993 Cumulative_SOC_deviation: 4.2386 Fuel Consumption: 112.6684\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 173 Total reward: -155.19880726386356 Explore P: 0.0186 SOC: 0.5994 Cumulative_SOC_deviation: 4.3194 Fuel Consumption: 112.0044\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 174 Total reward: -156.33684646282433 Explore P: 0.0183 SOC: 0.5978 Cumulative_SOC_deviation: 4.4723 Fuel Consumption: 111.6137\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 175 Total reward: -154.36069247521695 Explore P: 0.0181 SOC: 0.5974 Cumulative_SOC_deviation: 4.2771 Fuel Consumption: 111.5893\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 176 Total reward: -144.3610075261199 Explore P: 0.0179 SOC: 0.5934 Cumulative_SOC_deviation: 3.2777 Fuel Consumption: 111.5844\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 177 Total reward: -148.1329973018826 Explore P: 0.0177 SOC: 0.6028 Cumulative_SOC_deviation: 3.4561 Fuel Consumption: 113.5717\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 178 Total reward: -143.017545912806 Explore P: 0.0175 SOC: 0.6008 Cumulative_SOC_deviation: 2.8772 Fuel Consumption: 114.2455\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 179 Total reward: -143.95002326792493 Explore P: 0.0173 SOC: 0.5971 Cumulative_SOC_deviation: 3.2468 Fuel Consumption: 111.4819\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 180 Total reward: -141.5428412422958 Explore P: 0.0171 SOC: 0.5991 Cumulative_SOC_deviation: 2.9172 Fuel Consumption: 112.3706\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 181 Total reward: -134.15233430797238 Explore P: 0.0169 SOC: 0.6009 Cumulative_SOC_deviation: 2.2485 Fuel Consumption: 111.6675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 182 Total reward: -143.69270757716976 Explore P: 0.0167 SOC: 0.6002 Cumulative_SOC_deviation: 3.1918 Fuel Consumption: 111.7749\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 183 Total reward: -144.30248652270382 Explore P: 0.0165 SOC: 0.6000 Cumulative_SOC_deviation: 3.1209 Fuel Consumption: 113.0932\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 184 Total reward: -142.70086575486886 Explore P: 0.0163 SOC: 0.6029 Cumulative_SOC_deviation: 3.0023 Fuel Consumption: 112.6782\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 185 Total reward: -140.8895746771731 Explore P: 0.0162 SOC: 0.5997 Cumulative_SOC_deviation: 2.8524 Fuel Consumption: 112.3657\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 186 Total reward: -146.90406700133826 Explore P: 0.0160 SOC: 0.6006 Cumulative_SOC_deviation: 3.6780 Fuel Consumption: 110.1245\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 187 Total reward: -135.62797417818484 Explore P: 0.0158 SOC: 0.5985 Cumulative_SOC_deviation: 2.6031 Fuel Consumption: 109.5972\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 188 Total reward: -145.17130334059414 Explore P: 0.0157 SOC: 0.5982 Cumulative_SOC_deviation: 3.5555 Fuel Consumption: 109.6167\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 189 Total reward: -133.66580081829397 Explore P: 0.0155 SOC: 0.5979 Cumulative_SOC_deviation: 2.4152 Fuel Consumption: 109.5142\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 190 Total reward: -143.72722218607356 Explore P: 0.0154 SOC: 0.6033 Cumulative_SOC_deviation: 3.2040 Fuel Consumption: 111.6870\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 191 Total reward: -2033.9121386847837 Explore P: 0.0152 SOC: 0.9987 Cumulative_SOC_deviation: 187.7988 Fuel Consumption: 155.9242\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 192 Total reward: -152.9974447001801 Explore P: 0.0151 SOC: 0.5994 Cumulative_SOC_deviation: 4.0202 Fuel Consumption: 112.7954\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 193 Total reward: -142.1729037745528 Explore P: 0.0149 SOC: 0.5953 Cumulative_SOC_deviation: 3.0115 Fuel Consumption: 112.0581\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 194 Total reward: -148.46804432524564 Explore P: 0.0148 SOC: 0.5981 Cumulative_SOC_deviation: 3.7841 Fuel Consumption: 110.6274\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 195 Total reward: -147.76655319995157 Explore P: 0.0147 SOC: 0.6002 Cumulative_SOC_deviation: 3.6773 Fuel Consumption: 110.9936\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 196 Total reward: -132.12309578702894 Explore P: 0.0146 SOC: 0.6059 Cumulative_SOC_deviation: 2.2506 Fuel Consumption: 109.6167\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 197 Total reward: -140.425237180377 Explore P: 0.0144 SOC: 0.5994 Cumulative_SOC_deviation: 3.0901 Fuel Consumption: 109.5239\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 198 Total reward: -137.50155958479684 Explore P: 0.0143 SOC: 0.5995 Cumulative_SOC_deviation: 2.8705 Fuel Consumption: 108.7964\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 199 Total reward: -134.79641287761785 Explore P: 0.0142 SOC: 0.5997 Cumulative_SOC_deviation: 2.2953 Fuel Consumption: 111.8432\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 200 Total reward: -132.72896349110127 Explore P: 0.0141 SOC: 0.6000 Cumulative_SOC_deviation: 2.3361 Fuel Consumption: 109.3677\n"
     ]
    }
   ],
   "source": [
    "print(\"environment version: {}\".format(env.version)) \n",
    "\n",
    "# reward_factors = [10]\n",
    "num_trial = 3 \n",
    "results_dict = {} \n",
    "\n",
    "for trial in range(num_trial): \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    env, memory, primary_network, target_network = initialization_with_rewardFactor(10)\n",
    "    for episode in range(TOTAL_EPISODES): \n",
    "        state = env.reset() \n",
    "        avg_loss = 0 \n",
    "        total_reward = 0\n",
    "        cnt = 1 \n",
    "\n",
    "        while True:\n",
    "            action, action_continue = choose_action(state, primary_network, eps)\n",
    "            next_state, reward, done = env.step(action_continue)\n",
    "            total_reward += reward \n",
    "            if done: \n",
    "                next_state = None \n",
    "            memory.add_sample((state, action, reward, next_state))\n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                loss = train(primary_network, target_network, memory)\n",
    "                update_network(primary_network, target_network)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "            else: \n",
    "                loss = -1\n",
    "\n",
    "            avg_loss += loss \n",
    "            steps += 1 \n",
    "\n",
    "            if done: \n",
    "                if steps > DELAY_TRAINING: \n",
    "                    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "                    avg_loss /= cnt \n",
    "                    print('Episode: {}'.format(episode + 1),\n",
    "                          'Total reward: {}'.format(total_reward), \n",
    "                          'Explore P: {:.4f}'.format(eps), \n",
    "                          \"SOC: {:.4f}\".format(env.SOC), \n",
    "                         \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "                         \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "                         )\n",
    "                else: \n",
    "                    print(f\"Pre-training...Episode: {episode}\")\n",
    "                \n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_SOCs.append(env.SOC)\n",
    "                episode_FCs.append(env.fuel_consumption)\n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "            cnt += 1 \n",
    "    \n",
    "    results_dict[trial] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDQN3_3.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"DDQN3_3.pkl\", \"rb\") as f: \n",
    "#     data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
