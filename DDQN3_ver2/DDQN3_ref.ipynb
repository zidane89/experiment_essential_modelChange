{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "\n",
    "from vehicle_model_DDQN3_ref import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_e-4wd_Battery.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_id_75_110_Westinghouse.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATE_SIZE = env.calculation_comp[\"state_size\"]\n",
    "STATE_SIZE = 4\n",
    "ACTION_SIZE = env.calculation_comp[\"action_size\"] \n",
    "LEARNING_RATE = 0.00025 \n",
    "\n",
    "TOTAL_EPISODES = 200\n",
    "MAX_STEPS = 50000 \n",
    "\n",
    "GAMMA = 0.95 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "TAU = 0.001 \n",
    "DELAY_TRAINING = 3000 \n",
    "EPSILON_MIN_ITER = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "target_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "\n",
    "primary_network.compile(\n",
    "    loss=\"mse\", \n",
    "    optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    ")\n",
    "\n",
    "# for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "#     t.assign(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network(primary_network, target_network): \n",
    "    for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "        t.assign(t * (1 - TAU) + p * TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory: \n",
    "    def __init__(self, max_memory): \n",
    "        self.max_memory = max_memory \n",
    "        self._samples = [] \n",
    "        \n",
    "    def add_sample(self, sample): \n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self.max_memory: \n",
    "            self._samples.pop(0)\n",
    "        \n",
    "    def sample(self, no_samples): \n",
    "        if no_samples > len(self._samples): \n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else: \n",
    "            return random.sample(self._samples, no_samples)\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)\n",
    "    \n",
    "\n",
    "# memory = Memory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, primary_network, eps): \n",
    "    if random.random() < eps: \n",
    "        return random.randint(0, ACTION_SIZE - 1)\n",
    "    else: \n",
    "        return np.argmax(primary_network(np.array(state).reshape(1, -1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(primary_network, target_network, memory): \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    states = np.array([val[0] for val in batch]) \n",
    "    actions = np.array([val[1] for val in batch])\n",
    "    rewards = np.array([val[2] for val in batch])\n",
    "    next_states = np.array([np.zeros(STATE_SIZE) if val[3] is None else val[3]  \n",
    "                            for val in batch])\n",
    "    \n",
    "    prim_qt = primary_network(states)\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "    target_q = prim_qt.numpy() \n",
    "    updates = rewards \n",
    "    valid_idxs = next_states.sum(axis=1) != 0 \n",
    "    batch_idxs = np.arange(BATCH_SIZE)\n",
    "    prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "    q_from_target = target_network(next_states)\n",
    "    updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], \n",
    "                                                        prim_action_tp1[valid_idxs]]\n",
    "    \n",
    "    target_q[batch_idxs, actions] = updates \n",
    "    loss = primary_network.train_on_batch(states, target_q)\n",
    "    return loss \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_with_rewardFactor(reward_factor):\n",
    "    env = Environment(cell_model, drving_cycle, battery_path, motor_path, reward_factor)\n",
    "    \n",
    "    memory = Memory(10000)\n",
    "    \n",
    "    primary_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(),  \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    target_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    primary_network.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    "    )\n",
    "    return env, memory, primary_network, target_network \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment version: ref\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 0\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 1\n",
      "WARNING:tensorflow:Layer sequential_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer sequential_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 3 Total reward: -921.3366532834289 Explore P: 0.9217 SOC: 0.7919 Cumulative_SOC_deviation: 86.1252 Fuel Consumption: 60.0845\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 4 Total reward: -781.1100701583694 Explore P: 0.8970 SOC: 0.7577 Cumulative_SOC_deviation: 72.3582 Fuel Consumption: 57.5278\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 5 Total reward: -934.5759842783935 Explore P: 0.8730 SOC: 0.7769 Cumulative_SOC_deviation: 87.5530 Fuel Consumption: 59.0456\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 6 Total reward: -726.7350239553078 Explore P: 0.8496 SOC: 0.7365 Cumulative_SOC_deviation: 67.1174 Fuel Consumption: 55.5610\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 7 Total reward: -789.8855783715967 Explore P: 0.8269 SOC: 0.7536 Cumulative_SOC_deviation: 73.2745 Fuel Consumption: 57.1406\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 8 Total reward: -742.6068164342973 Explore P: 0.8048 SOC: 0.7136 Cumulative_SOC_deviation: 68.8703 Fuel Consumption: 53.9038\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 9 Total reward: -728.3170893082647 Explore P: 0.7832 SOC: 0.7074 Cumulative_SOC_deviation: 67.4849 Fuel Consumption: 53.4680\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 10 Total reward: -696.3369708203652 Explore P: 0.7623 SOC: 0.6786 Cumulative_SOC_deviation: 64.5155 Fuel Consumption: 51.1819\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 11 Total reward: -808.8539521004714 Explore P: 0.7419 SOC: 0.6817 Cumulative_SOC_deviation: 75.7436 Fuel Consumption: 51.4175\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 12 Total reward: -737.3562935696864 Explore P: 0.7221 SOC: 0.6772 Cumulative_SOC_deviation: 68.6509 Fuel Consumption: 50.8474\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 13 Total reward: -620.3204903785573 Explore P: 0.7028 SOC: 0.6844 Cumulative_SOC_deviation: 56.8700 Fuel Consumption: 51.6208\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 14 Total reward: -663.4082884694046 Explore P: 0.6840 SOC: 0.6618 Cumulative_SOC_deviation: 61.3522 Fuel Consumption: 49.8860\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 15 Total reward: -645.7377726264739 Explore P: 0.6658 SOC: 0.6850 Cumulative_SOC_deviation: 59.4291 Fuel Consumption: 51.4469\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 16 Total reward: -754.7197278378812 Explore P: 0.6480 SOC: 0.6671 Cumulative_SOC_deviation: 70.4630 Fuel Consumption: 50.0898\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 17 Total reward: -671.817303371824 Explore P: 0.6307 SOC: 0.6610 Cumulative_SOC_deviation: 62.2238 Fuel Consumption: 49.5794\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 18 Total reward: -627.5052810436565 Explore P: 0.6139 SOC: 0.6682 Cumulative_SOC_deviation: 57.7301 Fuel Consumption: 50.2043\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 19 Total reward: -805.7127051314252 Explore P: 0.5976 SOC: 0.6586 Cumulative_SOC_deviation: 75.6156 Fuel Consumption: 49.5566\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 20 Total reward: -545.5355763002283 Explore P: 0.5816 SOC: 0.6575 Cumulative_SOC_deviation: 49.6206 Fuel Consumption: 49.3296\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 21 Total reward: -745.3565526136869 Explore P: 0.5662 SOC: 0.7181 Cumulative_SOC_deviation: 69.1086 Fuel Consumption: 54.2702\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 22 Total reward: -705.2689442710807 Explore P: 0.5511 SOC: 0.6985 Cumulative_SOC_deviation: 65.2551 Fuel Consumption: 52.7179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 23 Total reward: -604.5522308258012 Explore P: 0.5364 SOC: 0.6245 Cumulative_SOC_deviation: 55.7760 Fuel Consumption: 46.7921\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 24 Total reward: -573.3155680752155 Explore P: 0.5222 SOC: 0.6602 Cumulative_SOC_deviation: 52.3934 Fuel Consumption: 49.3813\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 25 Total reward: -725.8742942714738 Explore P: 0.5083 SOC: 0.6645 Cumulative_SOC_deviation: 67.6025 Fuel Consumption: 49.8491\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 26 Total reward: -594.1333626721449 Explore P: 0.4948 SOC: 0.7108 Cumulative_SOC_deviation: 54.0334 Fuel Consumption: 53.7989\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 27 Total reward: -678.8181020590456 Explore P: 0.4817 SOC: 0.6754 Cumulative_SOC_deviation: 62.8135 Fuel Consumption: 50.6832\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 28 Total reward: -732.7687737494508 Explore P: 0.4689 SOC: 0.6771 Cumulative_SOC_deviation: 68.1780 Fuel Consumption: 50.9888\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 29 Total reward: -734.169345853169 Explore P: 0.4565 SOC: 0.6325 Cumulative_SOC_deviation: 68.6852 Fuel Consumption: 47.3172\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 30 Total reward: -650.9269096010629 Explore P: 0.4444 SOC: 0.6486 Cumulative_SOC_deviation: 60.2356 Fuel Consumption: 48.5709\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 31 Total reward: -548.7205697315972 Explore P: 0.4326 SOC: 0.7178 Cumulative_SOC_deviation: 49.5001 Fuel Consumption: 53.7193\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 32 Total reward: -752.7411397597615 Explore P: 0.4212 SOC: 0.7222 Cumulative_SOC_deviation: 69.8159 Fuel Consumption: 54.5824\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 33 Total reward: -484.71480390442366 Explore P: 0.4100 SOC: 0.6735 Cumulative_SOC_deviation: 43.4363 Fuel Consumption: 50.3518\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 34 Total reward: -523.507821810779 Explore P: 0.3992 SOC: 0.6549 Cumulative_SOC_deviation: 47.4955 Fuel Consumption: 48.5532\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 35 Total reward: -711.4358320669802 Explore P: 0.3887 SOC: 0.7407 Cumulative_SOC_deviation: 65.5546 Fuel Consumption: 55.8899\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 36 Total reward: -705.3709425639944 Explore P: 0.3784 SOC: 0.7204 Cumulative_SOC_deviation: 65.1060 Fuel Consumption: 54.3113\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 37 Total reward: -712.0975339664091 Explore P: 0.3684 SOC: 0.6988 Cumulative_SOC_deviation: 65.9609 Fuel Consumption: 52.4884\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 38 Total reward: -597.0777022141356 Explore P: 0.3587 SOC: 0.6832 Cumulative_SOC_deviation: 54.5953 Fuel Consumption: 51.1251\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 39 Total reward: -717.9133722359461 Explore P: 0.3493 SOC: 0.6122 Cumulative_SOC_deviation: 67.2372 Fuel Consumption: 45.5414\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 40 Total reward: -433.9681780151383 Explore P: 0.3401 SOC: 0.6454 Cumulative_SOC_deviation: 38.5865 Fuel Consumption: 48.1032\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 41 Total reward: -455.96991426688874 Explore P: 0.3311 SOC: 0.6664 Cumulative_SOC_deviation: 40.6204 Fuel Consumption: 49.7659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 42 Total reward: -528.2218750089339 Explore P: 0.3224 SOC: 0.6632 Cumulative_SOC_deviation: 47.8839 Fuel Consumption: 49.3828\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 43 Total reward: -642.5577387442607 Explore P: 0.3140 SOC: 0.7052 Cumulative_SOC_deviation: 58.9803 Fuel Consumption: 52.7549\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 44 Total reward: -787.9204644377 Explore P: 0.3057 SOC: 0.7909 Cumulative_SOC_deviation: 72.8348 Fuel Consumption: 59.5722\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 45 Total reward: -814.9887254151138 Explore P: 0.2977 SOC: 0.6279 Cumulative_SOC_deviation: 76.7720 Fuel Consumption: 47.2685\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 46 Total reward: -656.0446445223959 Explore P: 0.2899 SOC: 0.6962 Cumulative_SOC_deviation: 60.3768 Fuel Consumption: 52.2770\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 47 Total reward: -543.756077861241 Explore P: 0.2824 SOC: 0.6281 Cumulative_SOC_deviation: 49.6357 Fuel Consumption: 47.3988\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 48 Total reward: -831.2731398777586 Explore P: 0.2750 SOC: 0.6346 Cumulative_SOC_deviation: 78.3388 Fuel Consumption: 47.8848\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 49 Total reward: -620.1392468652787 Explore P: 0.2678 SOC: 0.6855 Cumulative_SOC_deviation: 56.8534 Fuel Consumption: 51.6051\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 50 Total reward: -578.0127790389149 Explore P: 0.2608 SOC: 0.6451 Cumulative_SOC_deviation: 52.9271 Fuel Consumption: 48.7422\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 51 Total reward: -559.8338600251236 Explore P: 0.2540 SOC: 0.6261 Cumulative_SOC_deviation: 51.2963 Fuel Consumption: 46.8712\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 52 Total reward: -441.18190481365684 Explore P: 0.2474 SOC: 0.6286 Cumulative_SOC_deviation: 39.4312 Fuel Consumption: 46.8702\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 53 Total reward: -618.1231578970202 Explore P: 0.2410 SOC: 0.6831 Cumulative_SOC_deviation: 56.6770 Fuel Consumption: 51.3532\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 54 Total reward: -436.5139221042158 Explore P: 0.2347 SOC: 0.6334 Cumulative_SOC_deviation: 38.9311 Fuel Consumption: 47.2026\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 55 Total reward: -477.49474358103134 Explore P: 0.2286 SOC: 0.6288 Cumulative_SOC_deviation: 43.0971 Fuel Consumption: 46.5241\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 56 Total reward: -562.7797512669891 Explore P: 0.2227 SOC: 0.6906 Cumulative_SOC_deviation: 51.0813 Fuel Consumption: 51.9664\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 57 Total reward: -552.9536153172999 Explore P: 0.2170 SOC: 0.6690 Cumulative_SOC_deviation: 50.2961 Fuel Consumption: 49.9930\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 58 Total reward: -626.95413640692 Explore P: 0.2114 SOC: 0.7608 Cumulative_SOC_deviation: 56.9499 Fuel Consumption: 57.4553\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 59 Total reward: -731.8401902123701 Explore P: 0.2059 SOC: 0.7769 Cumulative_SOC_deviation: 67.2943 Fuel Consumption: 58.8976\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 60 Total reward: -395.94923285678027 Explore P: 0.2006 SOC: 0.6550 Cumulative_SOC_deviation: 34.6898 Fuel Consumption: 49.0514\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 61 Total reward: -403.14709607767475 Explore P: 0.1954 SOC: 0.6550 Cumulative_SOC_deviation: 35.4235 Fuel Consumption: 48.9120\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 62 Total reward: -524.8336090290456 Explore P: 0.1904 SOC: 0.6719 Cumulative_SOC_deviation: 47.4167 Fuel Consumption: 50.6665\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 63 Total reward: -338.76283079429186 Explore P: 0.1855 SOC: 0.6325 Cumulative_SOC_deviation: 29.1560 Fuel Consumption: 47.2026\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 64 Total reward: -565.286194425423 Explore P: 0.1808 SOC: 0.6912 Cumulative_SOC_deviation: 51.3560 Fuel Consumption: 51.7262\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 65 Total reward: -524.7790397955341 Explore P: 0.1761 SOC: 0.6158 Cumulative_SOC_deviation: 47.9189 Fuel Consumption: 45.5896\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 66 Total reward: -532.1166614177887 Explore P: 0.1716 SOC: 0.7063 Cumulative_SOC_deviation: 47.8788 Fuel Consumption: 53.3286\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 67 Total reward: -521.5642611093167 Explore P: 0.1673 SOC: 0.6867 Cumulative_SOC_deviation: 47.0106 Fuel Consumption: 51.4586\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 68 Total reward: -470.4450830233547 Explore P: 0.1630 SOC: 0.6390 Cumulative_SOC_deviation: 42.2946 Fuel Consumption: 47.4991\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 69 Total reward: -489.617200694246 Explore P: 0.1589 SOC: 0.6322 Cumulative_SOC_deviation: 44.2599 Fuel Consumption: 47.0182\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 70 Total reward: -410.8467143568654 Explore P: 0.1548 SOC: 0.6304 Cumulative_SOC_deviation: 36.3979 Fuel Consumption: 46.8682\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 71 Total reward: -362.8559521649807 Explore P: 0.1509 SOC: 0.6212 Cumulative_SOC_deviation: 31.7146 Fuel Consumption: 45.7102\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 72 Total reward: -486.9038986896498 Explore P: 0.1471 SOC: 0.6147 Cumulative_SOC_deviation: 44.1410 Fuel Consumption: 45.4943\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 73 Total reward: -578.6611906082842 Explore P: 0.1434 SOC: 0.6241 Cumulative_SOC_deviation: 53.2737 Fuel Consumption: 45.9245\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 74 Total reward: -404.3925548365875 Explore P: 0.1398 SOC: 0.6254 Cumulative_SOC_deviation: 35.8104 Fuel Consumption: 46.2884\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 75 Total reward: -426.47080864644397 Explore P: 0.1362 SOC: 0.6209 Cumulative_SOC_deviation: 38.0479 Fuel Consumption: 45.9914\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 76 Total reward: -537.5578588521239 Explore P: 0.1328 SOC: 0.6572 Cumulative_SOC_deviation: 48.8002 Fuel Consumption: 49.5556\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 77 Total reward: -576.5063459292195 Explore P: 0.1295 SOC: 0.6523 Cumulative_SOC_deviation: 52.7702 Fuel Consumption: 48.8041\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 78 Total reward: -491.3328096501583 Explore P: 0.1263 SOC: 0.6374 Cumulative_SOC_deviation: 44.3588 Fuel Consumption: 47.7449\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 79 Total reward: -382.13220862122495 Explore P: 0.1231 SOC: 0.6180 Cumulative_SOC_deviation: 33.6227 Fuel Consumption: 45.9048\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 80 Total reward: -433.7908006034296 Explore P: 0.1200 SOC: 0.6300 Cumulative_SOC_deviation: 38.6724 Fuel Consumption: 47.0663\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 81 Total reward: -520.9896938457238 Explore P: 0.1171 SOC: 0.6212 Cumulative_SOC_deviation: 47.4753 Fuel Consumption: 46.2362\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 82 Total reward: -404.50424434391005 Explore P: 0.1142 SOC: 0.6107 Cumulative_SOC_deviation: 35.9257 Fuel Consumption: 45.2475\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 83 Total reward: -428.01390331985067 Explore P: 0.1113 SOC: 0.6056 Cumulative_SOC_deviation: 38.3199 Fuel Consumption: 44.8147\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 84 Total reward: -509.18362929192966 Explore P: 0.1086 SOC: 0.6443 Cumulative_SOC_deviation: 46.1335 Fuel Consumption: 47.8488\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 85 Total reward: -461.40710056104655 Explore P: 0.1059 SOC: 0.6380 Cumulative_SOC_deviation: 41.3911 Fuel Consumption: 47.4966\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 86 Total reward: -470.4166676561754 Explore P: 0.1033 SOC: 0.6531 Cumulative_SOC_deviation: 42.2144 Fuel Consumption: 48.2725\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 87 Total reward: -400.4518695182564 Explore P: 0.1008 SOC: 0.6299 Cumulative_SOC_deviation: 35.3866 Fuel Consumption: 46.5854\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 88 Total reward: -474.81192066925195 Explore P: 0.0983 SOC: 0.6617 Cumulative_SOC_deviation: 42.5745 Fuel Consumption: 49.0666\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 89 Total reward: -413.05552316065024 Explore P: 0.0960 SOC: 0.6296 Cumulative_SOC_deviation: 36.6332 Fuel Consumption: 46.7237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 90 Total reward: -430.0466930697976 Explore P: 0.0936 SOC: 0.6557 Cumulative_SOC_deviation: 38.1269 Fuel Consumption: 48.7777\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 91 Total reward: -387.18139259652645 Explore P: 0.0914 SOC: 0.6271 Cumulative_SOC_deviation: 34.0273 Fuel Consumption: 46.9082\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 92 Total reward: -349.7712603122939 Explore P: 0.0892 SOC: 0.6303 Cumulative_SOC_deviation: 30.3048 Fuel Consumption: 46.7237\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 93 Total reward: -364.15955720243664 Explore P: 0.0870 SOC: 0.6265 Cumulative_SOC_deviation: 31.7927 Fuel Consumption: 46.2322\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 94 Total reward: -315.3530926931585 Explore P: 0.0849 SOC: 0.6231 Cumulative_SOC_deviation: 26.9079 Fuel Consumption: 46.2737\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 95 Total reward: -358.6432289332972 Explore P: 0.0829 SOC: 0.6102 Cumulative_SOC_deviation: 31.2844 Fuel Consumption: 45.7989\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 96 Total reward: -458.5725480839531 Explore P: 0.0809 SOC: 0.6240 Cumulative_SOC_deviation: 41.1933 Fuel Consumption: 46.6396\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 97 Total reward: -332.56563366091694 Explore P: 0.0790 SOC: 0.6200 Cumulative_SOC_deviation: 28.6400 Fuel Consumption: 46.1658\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 98 Total reward: -385.043374313999 Explore P: 0.0771 SOC: 0.6171 Cumulative_SOC_deviation: 33.9312 Fuel Consumption: 45.7315\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 99 Total reward: -407.8397171891658 Explore P: 0.0753 SOC: 0.6256 Cumulative_SOC_deviation: 36.1222 Fuel Consumption: 46.6178\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 100 Total reward: -385.51681875669465 Explore P: 0.0735 SOC: 0.6054 Cumulative_SOC_deviation: 34.0925 Fuel Consumption: 44.5922\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 101 Total reward: -345.07215727786064 Explore P: 0.0718 SOC: 0.6049 Cumulative_SOC_deviation: 30.0482 Fuel Consumption: 44.5902\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 102 Total reward: -403.76546033190095 Explore P: 0.0701 SOC: 0.6202 Cumulative_SOC_deviation: 35.7902 Fuel Consumption: 45.8637\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 103 Total reward: -620.716342008431 Explore P: 0.0685 SOC: 0.6810 Cumulative_SOC_deviation: 56.9237 Fuel Consumption: 51.4789\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 104 Total reward: -335.13717435727534 Explore P: 0.0669 SOC: 0.6284 Cumulative_SOC_deviation: 28.9026 Fuel Consumption: 46.1115\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 105 Total reward: -378.0496488865195 Explore P: 0.0654 SOC: 0.6243 Cumulative_SOC_deviation: 33.2001 Fuel Consumption: 46.0482\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 106 Total reward: -461.6238168596618 Explore P: 0.0639 SOC: 0.6321 Cumulative_SOC_deviation: 41.4852 Fuel Consumption: 46.7719\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 107 Total reward: -451.38935028050645 Explore P: 0.0624 SOC: 0.6144 Cumulative_SOC_deviation: 40.6310 Fuel Consumption: 45.0792\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 108 Total reward: -415.3410267485858 Explore P: 0.0610 SOC: 0.6471 Cumulative_SOC_deviation: 36.7510 Fuel Consumption: 47.8305\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 109 Total reward: -477.66013885529145 Explore P: 0.0596 SOC: 0.6577 Cumulative_SOC_deviation: 42.9020 Fuel Consumption: 48.6404\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 110 Total reward: -384.096348066068 Explore P: 0.0583 SOC: 0.6080 Cumulative_SOC_deviation: 33.9562 Fuel Consumption: 44.5344\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 111 Total reward: -374.8088342508042 Explore P: 0.0570 SOC: 0.6308 Cumulative_SOC_deviation: 32.7924 Fuel Consumption: 46.8844\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 112 Total reward: -301.6760331325879 Explore P: 0.0557 SOC: 0.6259 Cumulative_SOC_deviation: 25.5553 Fuel Consumption: 46.1227\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 113 Total reward: -336.1111689062199 Explore P: 0.0545 SOC: 0.6168 Cumulative_SOC_deviation: 29.0617 Fuel Consumption: 45.4943\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 114 Total reward: -412.74617181723744 Explore P: 0.0533 SOC: 0.6206 Cumulative_SOC_deviation: 36.6644 Fuel Consumption: 46.1024\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 115 Total reward: -517.3752069718696 Explore P: 0.0521 SOC: 0.6438 Cumulative_SOC_deviation: 46.9456 Fuel Consumption: 47.9192\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 116 Total reward: -435.3821927417005 Explore P: 0.0510 SOC: 0.6290 Cumulative_SOC_deviation: 38.8463 Fuel Consumption: 46.9188\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 117 Total reward: -355.64910076857575 Explore P: 0.0498 SOC: 0.6358 Cumulative_SOC_deviation: 30.8306 Fuel Consumption: 47.3430\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 118 Total reward: -444.1111601012597 Explore P: 0.0488 SOC: 0.6157 Cumulative_SOC_deviation: 39.8445 Fuel Consumption: 45.6666\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 119 Total reward: -505.9438678747502 Explore P: 0.0477 SOC: 0.6263 Cumulative_SOC_deviation: 45.9338 Fuel Consumption: 46.6056\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 120 Total reward: -401.317819070393 Explore P: 0.0467 SOC: 0.6188 Cumulative_SOC_deviation: 35.5457 Fuel Consumption: 45.8612\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 121 Total reward: -436.58686174928613 Explore P: 0.0457 SOC: 0.6188 Cumulative_SOC_deviation: 39.0639 Fuel Consumption: 45.9478\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 122 Total reward: -415.06912090652986 Explore P: 0.0447 SOC: 0.6128 Cumulative_SOC_deviation: 36.9831 Fuel Consumption: 45.2378\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 123 Total reward: -355.52771729433533 Explore P: 0.0438 SOC: 0.6117 Cumulative_SOC_deviation: 31.0020 Fuel Consumption: 45.5080\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 124 Total reward: -377.544657709948 Explore P: 0.0429 SOC: 0.6154 Cumulative_SOC_deviation: 33.1870 Fuel Consumption: 45.6742\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 125 Total reward: -379.07843580570477 Explore P: 0.0420 SOC: 0.6075 Cumulative_SOC_deviation: 33.3662 Fuel Consumption: 45.4162\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 126 Total reward: -425.24128575542244 Explore P: 0.0411 SOC: 0.6407 Cumulative_SOC_deviation: 37.7545 Fuel Consumption: 47.6962\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 127 Total reward: -430.858370378136 Explore P: 0.0403 SOC: 0.6217 Cumulative_SOC_deviation: 38.4531 Fuel Consumption: 46.3274\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 128 Total reward: -402.7168081027941 Explore P: 0.0395 SOC: 0.6182 Cumulative_SOC_deviation: 35.6320 Fuel Consumption: 46.3963\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 129 Total reward: -445.4326736654054 Explore P: 0.0387 SOC: 0.6274 Cumulative_SOC_deviation: 39.8466 Fuel Consumption: 46.9665\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 130 Total reward: -384.33509524460555 Explore P: 0.0379 SOC: 0.6090 Cumulative_SOC_deviation: 33.9257 Fuel Consumption: 45.0777\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 131 Total reward: -393.57203245189123 Explore P: 0.0371 SOC: 0.6020 Cumulative_SOC_deviation: 34.8984 Fuel Consumption: 44.5876\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 132 Total reward: -381.07046622298503 Explore P: 0.0364 SOC: 0.6190 Cumulative_SOC_deviation: 33.5157 Fuel Consumption: 45.9134\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 133 Total reward: -360.3469531626198 Explore P: 0.0357 SOC: 0.6229 Cumulative_SOC_deviation: 31.3836 Fuel Consumption: 46.5114\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 134 Total reward: -349.17876461844935 Explore P: 0.0350 SOC: 0.6261 Cumulative_SOC_deviation: 30.2455 Fuel Consumption: 46.7237\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 135 Total reward: -390.4655487128742 Explore P: 0.0343 SOC: 0.6126 Cumulative_SOC_deviation: 34.4906 Fuel Consumption: 45.5591\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 136 Total reward: -386.9264403873234 Explore P: 0.0336 SOC: 0.6216 Cumulative_SOC_deviation: 34.0671 Fuel Consumption: 46.2550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 137 Total reward: -383.79604581310656 Explore P: 0.0330 SOC: 0.6244 Cumulative_SOC_deviation: 33.7440 Fuel Consumption: 46.3563\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 138 Total reward: -316.0703467437764 Explore P: 0.0324 SOC: 0.6125 Cumulative_SOC_deviation: 27.0658 Fuel Consumption: 45.4122\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 139 Total reward: -326.9926645628037 Explore P: 0.0318 SOC: 0.6094 Cumulative_SOC_deviation: 28.1783 Fuel Consumption: 45.2100\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 140 Total reward: -336.1030203365167 Explore P: 0.0312 SOC: 0.6471 Cumulative_SOC_deviation: 28.8064 Fuel Consumption: 48.0393\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 141 Total reward: -677.6014285386021 Explore P: 0.0306 SOC: 0.7090 Cumulative_SOC_deviation: 62.4293 Fuel Consumption: 53.3083\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 142 Total reward: -305.45062577935863 Explore P: 0.0301 SOC: 0.6100 Cumulative_SOC_deviation: 26.0269 Fuel Consumption: 45.1821\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 143 Total reward: -312.39153681084076 Explore P: 0.0295 SOC: 0.5972 Cumulative_SOC_deviation: 26.8045 Fuel Consumption: 44.3469\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 144 Total reward: -415.5588350168391 Explore P: 0.0290 SOC: 0.6337 Cumulative_SOC_deviation: 36.8449 Fuel Consumption: 47.1094\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 145 Total reward: -499.5269172778379 Explore P: 0.0285 SOC: 0.6272 Cumulative_SOC_deviation: 45.2404 Fuel Consumption: 47.1231\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 146 Total reward: -427.3329310852214 Explore P: 0.0280 SOC: 0.6330 Cumulative_SOC_deviation: 38.0005 Fuel Consumption: 47.3278\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 147 Total reward: -334.32919000442286 Explore P: 0.0275 SOC: 0.6125 Cumulative_SOC_deviation: 28.8549 Fuel Consumption: 45.7801\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 148 Total reward: -439.116542311394 Explore P: 0.0270 SOC: 0.6126 Cumulative_SOC_deviation: 39.3100 Fuel Consumption: 46.0168\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 149 Total reward: -391.0219551569871 Explore P: 0.0265 SOC: 0.6425 Cumulative_SOC_deviation: 34.3083 Fuel Consumption: 47.9385\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 150 Total reward: -321.02407669803455 Explore P: 0.0261 SOC: 0.6089 Cumulative_SOC_deviation: 27.5411 Fuel Consumption: 45.6129\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 151 Total reward: -389.8374181288779 Explore P: 0.0257 SOC: 0.6268 Cumulative_SOC_deviation: 34.2835 Fuel Consumption: 47.0020\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 152 Total reward: -310.93141977246427 Explore P: 0.0252 SOC: 0.6174 Cumulative_SOC_deviation: 26.5212 Fuel Consumption: 45.7198\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 153 Total reward: -267.9065487051295 Explore P: 0.0248 SOC: 0.5979 Cumulative_SOC_deviation: 22.3543 Fuel Consumption: 44.3636\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 154 Total reward: -350.7123350463506 Explore P: 0.0244 SOC: 0.6191 Cumulative_SOC_deviation: 30.4820 Fuel Consumption: 45.8921\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 155 Total reward: -328.0335070301853 Explore P: 0.0240 SOC: 0.6194 Cumulative_SOC_deviation: 28.1845 Fuel Consumption: 46.1886\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 156 Total reward: -322.00460062158214 Explore P: 0.0237 SOC: 0.6303 Cumulative_SOC_deviation: 27.4823 Fuel Consumption: 47.1814\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 157 Total reward: -345.97951118033023 Explore P: 0.0233 SOC: 0.6314 Cumulative_SOC_deviation: 29.9163 Fuel Consumption: 46.8170\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 158 Total reward: -507.8313032606634 Explore P: 0.0229 SOC: 0.6806 Cumulative_SOC_deviation: 45.6673 Fuel Consumption: 51.1581\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 159 Total reward: -306.7073027923399 Explore P: 0.0226 SOC: 0.6072 Cumulative_SOC_deviation: 26.1741 Fuel Consumption: 44.9662\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 160 Total reward: -327.84085494358317 Explore P: 0.0222 SOC: 0.6049 Cumulative_SOC_deviation: 28.2690 Fuel Consumption: 45.1512\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 161 Total reward: -340.2830796420498 Explore P: 0.0219 SOC: 0.6000 Cumulative_SOC_deviation: 29.5787 Fuel Consumption: 44.4964\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 162 Total reward: -329.43354993743293 Explore P: 0.0216 SOC: 0.6575 Cumulative_SOC_deviation: 28.0373 Fuel Consumption: 49.0610\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 163 Total reward: -340.83819686612355 Explore P: 0.0213 SOC: 0.6327 Cumulative_SOC_deviation: 29.3531 Fuel Consumption: 47.3075\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 164 Total reward: -357.383068068361 Explore P: 0.0210 SOC: 0.6035 Cumulative_SOC_deviation: 31.2241 Fuel Consumption: 45.1421\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 165 Total reward: -323.72888685373266 Explore P: 0.0207 SOC: 0.6130 Cumulative_SOC_deviation: 27.8154 Fuel Consumption: 45.5749\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 166 Total reward: -337.4095095736828 Explore P: 0.0204 SOC: 0.6352 Cumulative_SOC_deviation: 29.0274 Fuel Consumption: 47.1352\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 167 Total reward: -361.68654620453384 Explore P: 0.0201 SOC: 0.6355 Cumulative_SOC_deviation: 31.4394 Fuel Consumption: 47.2923\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 168 Total reward: -328.464142565187 Explore P: 0.0198 SOC: 0.6111 Cumulative_SOC_deviation: 28.2833 Fuel Consumption: 45.6316\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 169 Total reward: -307.062171920126 Explore P: 0.0196 SOC: 0.6308 Cumulative_SOC_deviation: 26.0403 Fuel Consumption: 46.6589\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 170 Total reward: -364.3157679488744 Explore P: 0.0193 SOC: 0.6263 Cumulative_SOC_deviation: 31.7586 Fuel Consumption: 46.7293\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 171 Total reward: -348.10667514515404 Explore P: 0.0190 SOC: 0.6279 Cumulative_SOC_deviation: 30.1139 Fuel Consumption: 46.9680\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 172 Total reward: -348.0592841496871 Explore P: 0.0188 SOC: 0.6055 Cumulative_SOC_deviation: 30.2983 Fuel Consumption: 45.0767\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 173 Total reward: -395.4124537244681 Explore P: 0.0186 SOC: 0.6200 Cumulative_SOC_deviation: 34.9098 Fuel Consumption: 46.3148\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 174 Total reward: -449.86296348179894 Explore P: 0.0183 SOC: 0.6104 Cumulative_SOC_deviation: 40.4619 Fuel Consumption: 45.2439\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 175 Total reward: -349.1255604203849 Explore P: 0.0181 SOC: 0.6135 Cumulative_SOC_deviation: 30.3201 Fuel Consumption: 45.9245\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 176 Total reward: -370.6798053000925 Explore P: 0.0179 SOC: 0.6132 Cumulative_SOC_deviation: 32.4794 Fuel Consumption: 45.8860\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 177 Total reward: -449.45880159015576 Explore P: 0.0177 SOC: 0.6247 Cumulative_SOC_deviation: 40.2676 Fuel Consumption: 46.7825\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 178 Total reward: -366.348071008192 Explore P: 0.0175 SOC: 0.6284 Cumulative_SOC_deviation: 31.9516 Fuel Consumption: 46.8317\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 179 Total reward: -379.73565406842226 Explore P: 0.0173 SOC: 0.6095 Cumulative_SOC_deviation: 33.4241 Fuel Consumption: 45.4948\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 180 Total reward: -337.7922216082762 Explore P: 0.0171 SOC: 0.6136 Cumulative_SOC_deviation: 29.2182 Fuel Consumption: 45.6098\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 181 Total reward: -367.9471269360704 Explore P: 0.0169 SOC: 0.6259 Cumulative_SOC_deviation: 32.1226 Fuel Consumption: 46.7207\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 182 Total reward: -399.2680368898437 Explore P: 0.0167 SOC: 0.6120 Cumulative_SOC_deviation: 35.3595 Fuel Consumption: 45.6727\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 183 Total reward: -398.9876745718582 Explore P: 0.0165 SOC: 0.6145 Cumulative_SOC_deviation: 35.3182 Fuel Consumption: 45.8054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 184 Total reward: -332.29133271669906 Explore P: 0.0163 SOC: 0.6094 Cumulative_SOC_deviation: 28.7137 Fuel Consumption: 45.1542\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 185 Total reward: -352.80210390702564 Explore P: 0.0162 SOC: 0.6238 Cumulative_SOC_deviation: 30.6205 Fuel Consumption: 46.5975\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 186 Total reward: -275.53671741378116 Explore P: 0.0160 SOC: 0.6140 Cumulative_SOC_deviation: 22.9659 Fuel Consumption: 45.8774\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 187 Total reward: -358.65707000289734 Explore P: 0.0158 SOC: 0.6086 Cumulative_SOC_deviation: 31.3335 Fuel Consumption: 45.3220\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 188 Total reward: -300.3624048361761 Explore P: 0.0157 SOC: 0.6217 Cumulative_SOC_deviation: 25.4073 Fuel Consumption: 46.2889\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 189 Total reward: -353.8306842678796 Explore P: 0.0155 SOC: 0.6104 Cumulative_SOC_deviation: 30.8259 Fuel Consumption: 45.5713\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 190 Total reward: -386.43196926060904 Explore P: 0.0154 SOC: 0.6301 Cumulative_SOC_deviation: 33.9418 Fuel Consumption: 47.0141\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 191 Total reward: -284.7289624377026 Explore P: 0.0152 SOC: 0.6109 Cumulative_SOC_deviation: 23.9194 Fuel Consumption: 45.5348\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 192 Total reward: -357.9100074391522 Explore P: 0.0151 SOC: 0.6170 Cumulative_SOC_deviation: 31.1655 Fuel Consumption: 46.2550\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 193 Total reward: -350.30267505773264 Explore P: 0.0149 SOC: 0.6029 Cumulative_SOC_deviation: 30.5179 Fuel Consumption: 45.1238\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 194 Total reward: -332.57409357915196 Explore P: 0.0148 SOC: 0.6260 Cumulative_SOC_deviation: 28.5777 Fuel Consumption: 46.7972\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 195 Total reward: -325.7756607880424 Explore P: 0.0147 SOC: 0.6076 Cumulative_SOC_deviation: 28.0395 Fuel Consumption: 45.3808\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 196 Total reward: -350.6396519224906 Explore P: 0.0146 SOC: 0.6081 Cumulative_SOC_deviation: 30.5433 Fuel Consumption: 45.2069\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 197 Total reward: -337.12548149735585 Explore P: 0.0144 SOC: 0.6101 Cumulative_SOC_deviation: 29.1618 Fuel Consumption: 45.5075\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 198 Total reward: -299.4577741023756 Explore P: 0.0143 SOC: 0.6169 Cumulative_SOC_deviation: 25.3466 Fuel Consumption: 45.9919\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 199 Total reward: -306.12518915302076 Explore P: 0.0142 SOC: 0.6065 Cumulative_SOC_deviation: 26.0997 Fuel Consumption: 45.1284\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 200 Total reward: -336.45537007714364 Explore P: 0.0141 SOC: 0.6091 Cumulative_SOC_deviation: 29.1029 Fuel Consumption: 45.4264\n"
     ]
    }
   ],
   "source": [
    "print(\"environment version: {}\".format(env.version)) \n",
    "\n",
    " \n",
    "reward_factors = [10]\n",
    "results_dict = {} \n",
    "\n",
    "for reward_factor in reward_factors: \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    env, memory, primary_network, target_network = initialization_with_rewardFactor(reward_factor)\n",
    "    for episode in range(TOTAL_EPISODES): \n",
    "        state = env.reset() \n",
    "        avg_loss = 0 \n",
    "        total_reward = 0\n",
    "        cnt = 1 \n",
    "\n",
    "        while True:\n",
    "            action = choose_action(state, primary_network, eps)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward \n",
    "            if done: \n",
    "                next_state = None \n",
    "            memory.add_sample((state, action, reward, next_state))\n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                loss = train(primary_network, target_network, memory)\n",
    "                update_network(primary_network, target_network)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "            else: \n",
    "                loss = -1\n",
    "\n",
    "            avg_loss += loss \n",
    "            steps += 1 \n",
    "\n",
    "            if done: \n",
    "                if steps > DELAY_TRAINING: \n",
    "                    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "                    avg_loss /= cnt \n",
    "                    print('Episode: {}'.format(episode + 1),\n",
    "                          'Total reward: {}'.format(total_reward), \n",
    "                          'Explore P: {:.4f}'.format(eps), \n",
    "                          \"SOC: {:.4f}\".format(env.SOC), \n",
    "                         \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "                         \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "                         )\n",
    "                else: \n",
    "                    print(f\"Pre-training...Episode: {episode}\")\n",
    "                \n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_SOCs.append(env.SOC)\n",
    "                episode_FCs.append(env.fuel_consumption)\n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "            cnt += 1 \n",
    "    \n",
    "    results_dict[reward_factor] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDQN3_norm.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/replay_memory_size_effect.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-cc692a8a37e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"results/replay_memory_size_effect.pkl\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/replay_memory_size_effect.pkl'"
     ]
    }
   ],
   "source": [
    "# with open(\"results/replay_memory_size_effect.pkl\", \"rb\") as f: \n",
    "#     data = pickle.load(f)\n",
    "    \n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
