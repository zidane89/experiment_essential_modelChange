{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "import scipy.io as sio\n",
    "\n",
    "from vehicle_model_variant_1 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driving_cycle_path = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "driving_cycle = sio.loadmat(driving_cycle_path)\n",
    "driving_cycle = driving_cycle[\"sch_cycle\"][:, 1]\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_nimh_6_240_panasonic_MY01_Prius.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_pm_95_145_X2.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, driving_cycle, battery_path, motor_path, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATE_SIZE = env.calculation_comp[\"state_size\"]\n",
    "STATE_SIZE = 4\n",
    "ACTION_SIZE = env.calculation_comp[\"action_size\"] \n",
    "LEARNING_RATE = 0.00025 \n",
    "\n",
    "TOTAL_EPISODES = 200\n",
    "MAX_STEPS = 50000 \n",
    "\n",
    "GAMMA = 0.95 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "TAU = 0.001 \n",
    "DELAY_TRAINING = 3000 \n",
    "EPSILON_MIN_ITER = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "target_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "\n",
    "primary_network.compile(\n",
    "    loss=\"mse\", \n",
    "    optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    ")\n",
    "\n",
    "# for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "#     t.assign(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network(primary_network, target_network): \n",
    "    for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "        t.assign(t * (1 - TAU) + p * TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory: \n",
    "    def __init__(self, max_memory): \n",
    "        self.max_memory = max_memory \n",
    "        self._samples = [] \n",
    "        \n",
    "    def add_sample(self, sample): \n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self.max_memory: \n",
    "            self._samples.pop(0)\n",
    "        \n",
    "    def sample(self, no_samples): \n",
    "        if no_samples > len(self._samples): \n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else: \n",
    "            return random.sample(self._samples, no_samples)\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)\n",
    "    \n",
    "\n",
    "# memory = Memory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, primary_network, eps):\n",
    "    j_min = state[-2]\n",
    "    j_max = state[-1]\n",
    "    \n",
    "    if random.random() < eps:\n",
    "        action = random.randint(0, ACTION_SIZE - 1) \n",
    "        action_continue = np.linspace(j_min, j_max, ACTION_SIZE)[action]\n",
    "        return action, action_continue \n",
    "    else: \n",
    "        action = np.argmax(primary_network(np.array(state).reshape(1, -1)))\n",
    "        action_continue = np.linspace(j_min, j_max, ACTION_SIZE)[action]\n",
    "        return action, action_continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(primary_network, target_network, memory): \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    states = np.array([val[0] for val in batch]) \n",
    "    actions = np.array([val[1] for val in batch])\n",
    "    rewards = np.array([val[2] for val in batch])\n",
    "    next_states = np.array([np.zeros(STATE_SIZE) if val[3] is None else val[3]  \n",
    "                            for val in batch])\n",
    "    \n",
    "    prim_qt = primary_network(states)\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "    target_q = prim_qt.numpy() \n",
    "    updates = rewards \n",
    "    valid_idxs = next_states.sum(axis=1) != 0 \n",
    "    batch_idxs = np.arange(BATCH_SIZE)\n",
    "    prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "    q_from_target = target_network(next_states)\n",
    "    updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], \n",
    "                                                        prim_action_tp1[valid_idxs]]\n",
    "    \n",
    "    target_q[batch_idxs, actions] = updates \n",
    "    loss = primary_network.train_on_batch(states, target_q)\n",
    "    return loss \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_with_rewardFactor(reward_factor):\n",
    "    env = Environment(cell_model, driving_cycle, battery_path, motor_path, reward_factor)\n",
    "    \n",
    "    memory = Memory(10000)\n",
    "    \n",
    "    primary_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(),  \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    target_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    primary_network.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    "    )\n",
    "    return env, memory, primary_network, target_network \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment version: 1\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 0\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 1\n",
      "WARNING:tensorflow:Layer sequential_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer sequential_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 3 Total reward: -5047.567980420613 Explore P: 0.9217 SOC: 1.0000 Cumulative_SOC_deviation: 485.8036 Fuel Consumption: 189.5319\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 4 Total reward: -4897.923837557252 Explore P: 0.8970 SOC: 1.0000 Cumulative_SOC_deviation: 471.2928 Fuel Consumption: 184.9958\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 5 Total reward: -4959.923804174062 Explore P: 0.8730 SOC: 1.0000 Cumulative_SOC_deviation: 476.9547 Fuel Consumption: 190.3766\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 6 Total reward: -5047.876104453785 Explore P: 0.8496 SOC: 0.9999 Cumulative_SOC_deviation: 485.5034 Fuel Consumption: 192.8423\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 7 Total reward: -5027.370403637389 Explore P: 0.8269 SOC: 1.0000 Cumulative_SOC_deviation: 483.0671 Fuel Consumption: 196.6997\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 8 Total reward: -5141.387302572715 Explore P: 0.8048 SOC: 1.0000 Cumulative_SOC_deviation: 495.5337 Fuel Consumption: 186.0505\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 9 Total reward: -5036.590204738034 Explore P: 0.7832 SOC: 1.0000 Cumulative_SOC_deviation: 484.5227 Fuel Consumption: 191.3629\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 10 Total reward: -4960.34515098534 Explore P: 0.7623 SOC: 1.0000 Cumulative_SOC_deviation: 477.0144 Fuel Consumption: 190.2008\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 11 Total reward: -5020.288020354194 Explore P: 0.7419 SOC: 0.9997 Cumulative_SOC_deviation: 482.8178 Fuel Consumption: 192.1099\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 12 Total reward: -5139.639881186111 Explore P: 0.7221 SOC: 1.0000 Cumulative_SOC_deviation: 495.0689 Fuel Consumption: 188.9508\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 13 Total reward: -4944.346541835996 Explore P: 0.7028 SOC: 1.0000 Cumulative_SOC_deviation: 475.4903 Fuel Consumption: 189.4440\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 14 Total reward: -5067.527799613525 Explore P: 0.6840 SOC: 1.0000 Cumulative_SOC_deviation: 487.9549 Fuel Consumption: 187.9792\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 15 Total reward: -5101.522972254553 Explore P: 0.6658 SOC: 1.0000 Cumulative_SOC_deviation: 491.0624 Fuel Consumption: 190.8990\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 16 Total reward: -4996.481711890681 Explore P: 0.6480 SOC: 1.0000 Cumulative_SOC_deviation: 480.8366 Fuel Consumption: 188.1159\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 17 Total reward: -4979.806435331897 Explore P: 0.6307 SOC: 1.0000 Cumulative_SOC_deviation: 479.3834 Fuel Consumption: 185.9724\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 18 Total reward: -4949.388740041787 Explore P: 0.6139 SOC: 0.9987 Cumulative_SOC_deviation: 476.1087 Fuel Consumption: 188.3014\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 19 Total reward: -5012.543202278001 Explore P: 0.5976 SOC: 1.0000 Cumulative_SOC_deviation: 481.2382 Fuel Consumption: 200.1615\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 20 Total reward: -5120.892116746051 Explore P: 0.5816 SOC: 1.0000 Cumulative_SOC_deviation: 491.8787 Fuel Consumption: 202.1048\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 21 Total reward: -5164.547842001864 Explore P: 0.5662 SOC: 1.0000 Cumulative_SOC_deviation: 496.1784 Fuel Consumption: 202.7640\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 22 Total reward: -5126.407572264272 Explore P: 0.5511 SOC: 1.0000 Cumulative_SOC_deviation: 493.1607 Fuel Consumption: 194.8003\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 23 Total reward: -5204.945290358959 Explore P: 0.5364 SOC: 0.9995 Cumulative_SOC_deviation: 501.0785 Fuel Consumption: 194.1607\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 24 Total reward: -5076.50615813584 Explore P: 0.5222 SOC: 0.9995 Cumulative_SOC_deviation: 488.3664 Fuel Consumption: 192.8423\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 25 Total reward: -4973.878564522304 Explore P: 0.5083 SOC: 0.9989 Cumulative_SOC_deviation: 479.1847 Fuel Consumption: 182.0320\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 26 Total reward: -5071.462332923901 Explore P: 0.4948 SOC: 0.9988 Cumulative_SOC_deviation: 488.4284 Fuel Consumption: 187.1784\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 27 Total reward: -5042.174641519059 Explore P: 0.4817 SOC: 0.9998 Cumulative_SOC_deviation: 484.9098 Fuel Consumption: 193.0767\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 28 Total reward: -4915.072863163888 Explore P: 0.4689 SOC: 1.0000 Cumulative_SOC_deviation: 472.4950 Fuel Consumption: 190.1227\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 29 Total reward: -5067.39820665763 Explore P: 0.4565 SOC: 1.0000 Cumulative_SOC_deviation: 486.8150 Fuel Consumption: 199.2485\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 30 Total reward: -5061.650387559758 Explore P: 0.4444 SOC: 1.0000 Cumulative_SOC_deviation: 487.0283 Fuel Consumption: 191.3678\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 31 Total reward: -5214.244643012896 Explore P: 0.4326 SOC: 0.9998 Cumulative_SOC_deviation: 501.2018 Fuel Consumption: 202.2269\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 32 Total reward: -5061.729279898639 Explore P: 0.4212 SOC: 0.9985 Cumulative_SOC_deviation: 487.5103 Fuel Consumption: 186.6267\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 33 Total reward: -5135.603946504766 Explore P: 0.4100 SOC: 1.0000 Cumulative_SOC_deviation: 493.7024 Fuel Consumption: 198.5795\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 34 Total reward: -5127.190049604395 Explore P: 0.3992 SOC: 1.0000 Cumulative_SOC_deviation: 493.0608 Fuel Consumption: 196.5825\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 35 Total reward: -5140.000347364245 Explore P: 0.3887 SOC: 0.9993 Cumulative_SOC_deviation: 495.7602 Fuel Consumption: 182.3983\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 36 Total reward: -5125.482785929019 Explore P: 0.3784 SOC: 1.0000 Cumulative_SOC_deviation: 492.9940 Fuel Consumption: 195.5425\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 37 Total reward: -5110.67841630249 Explore P: 0.3684 SOC: 1.0000 Cumulative_SOC_deviation: 491.4657 Fuel Consumption: 196.0210\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 38 Total reward: -5086.902072525842 Explore P: 0.3587 SOC: 1.0000 Cumulative_SOC_deviation: 489.0837 Fuel Consumption: 196.0649\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 39 Total reward: -5136.011147303 Explore P: 0.3493 SOC: 0.9999 Cumulative_SOC_deviation: 494.0674 Fuel Consumption: 195.3374\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 40 Total reward: -5126.852337374944 Explore P: 0.3401 SOC: 1.0000 Cumulative_SOC_deviation: 493.2823 Fuel Consumption: 194.0288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 41 Total reward: -5192.017379876116 Explore P: 0.3311 SOC: 1.0000 Cumulative_SOC_deviation: 499.1914 Fuel Consumption: 200.1029\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 42 Total reward: -5092.2055850297165 Explore P: 0.3224 SOC: 1.0000 Cumulative_SOC_deviation: 490.1311 Fuel Consumption: 190.8942\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 43 Total reward: -5172.928161216047 Explore P: 0.3140 SOC: 0.9994 Cumulative_SOC_deviation: 498.6565 Fuel Consumption: 186.3630\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 44 Total reward: -4732.746053142113 Explore P: 0.3057 SOC: 1.0000 Cumulative_SOC_deviation: 455.3546 Fuel Consumption: 179.2001\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 45 Total reward: -4995.249084327413 Explore P: 0.2977 SOC: 0.9998 Cumulative_SOC_deviation: 480.8974 Fuel Consumption: 186.2751\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 46 Total reward: -4980.157260211343 Explore P: 0.2899 SOC: 0.9999 Cumulative_SOC_deviation: 479.7788 Fuel Consumption: 182.3690\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 47 Total reward: -4933.678391461853 Explore P: 0.2824 SOC: 0.9983 Cumulative_SOC_deviation: 474.7892 Fuel Consumption: 185.7868\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 48 Total reward: -4970.147738667285 Explore P: 0.2750 SOC: 1.0000 Cumulative_SOC_deviation: 478.7540 Fuel Consumption: 182.6082\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 49 Total reward: -5073.032504541627 Explore P: 0.2678 SOC: 1.0000 Cumulative_SOC_deviation: 488.3296 Fuel Consumption: 189.7370\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 50 Total reward: -5190.871824443405 Explore P: 0.2608 SOC: 0.9989 Cumulative_SOC_deviation: 500.0905 Fuel Consumption: 189.9664\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 51 Total reward: -5161.913136468417 Explore P: 0.2540 SOC: 1.0000 Cumulative_SOC_deviation: 496.6004 Fuel Consumption: 195.9087\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 52 Total reward: -5092.301111396438 Explore P: 0.2474 SOC: 1.0000 Cumulative_SOC_deviation: 490.7974 Fuel Consumption: 184.3269\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 53 Total reward: -5152.816151223272 Explore P: 0.2410 SOC: 1.0000 Cumulative_SOC_deviation: 495.4505 Fuel Consumption: 198.3110\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 54 Total reward: -5053.56512846211 Explore P: 0.2347 SOC: 1.0000 Cumulative_SOC_deviation: 485.8862 Fuel Consumption: 194.7027\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 55 Total reward: -5164.18938628355 Explore P: 0.2286 SOC: 1.0000 Cumulative_SOC_deviation: 496.2124 Fuel Consumption: 202.0658\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 56 Total reward: -5093.475168984954 Explore P: 0.2227 SOC: 1.0000 Cumulative_SOC_deviation: 490.3870 Fuel Consumption: 189.6051\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 57 Total reward: -5138.478640805664 Explore P: 0.2170 SOC: 1.0000 Cumulative_SOC_deviation: 493.9396 Fuel Consumption: 199.0824\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 58 Total reward: -4833.531691980472 Explore P: 0.2114 SOC: 0.9995 Cumulative_SOC_deviation: 464.3565 Fuel Consumption: 189.9664\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 59 Total reward: -4985.515663482667 Explore P: 0.2059 SOC: 0.9999 Cumulative_SOC_deviation: 480.6931 Fuel Consumption: 178.5849\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 60 Total reward: -4877.4216830189325 Explore P: 0.2006 SOC: 1.0000 Cumulative_SOC_deviation: 468.8554 Fuel Consumption: 188.8678\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 61 Total reward: -5066.362884651527 Explore P: 0.1954 SOC: 1.0000 Cumulative_SOC_deviation: 487.7114 Fuel Consumption: 189.2487\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 62 Total reward: -4976.670336340425 Explore P: 0.1904 SOC: 1.0000 Cumulative_SOC_deviation: 479.2544 Fuel Consumption: 184.1267\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 63 Total reward: -4922.118698922467 Explore P: 0.1855 SOC: 0.9964 Cumulative_SOC_deviation: 474.7914 Fuel Consumption: 174.2051\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 64 Total reward: -4928.4290841221155 Explore P: 0.1808 SOC: 0.9995 Cumulative_SOC_deviation: 474.6407 Fuel Consumption: 182.0223\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 65 Total reward: -4914.932612879328 Explore P: 0.1761 SOC: 0.9998 Cumulative_SOC_deviation: 472.8555 Fuel Consumption: 186.3777\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 66 Total reward: -5059.965632785983 Explore P: 0.1716 SOC: 1.0000 Cumulative_SOC_deviation: 487.1161 Fuel Consumption: 188.8044\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 67 Total reward: -5192.082652557018 Explore P: 0.1673 SOC: 1.0000 Cumulative_SOC_deviation: 498.5935 Fuel Consumption: 206.1477\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 68 Total reward: -5170.772250543886 Explore P: 0.1630 SOC: 0.9963 Cumulative_SOC_deviation: 497.6607 Fuel Consumption: 194.1656\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 69 Total reward: -5189.47248766049 Explore P: 0.1589 SOC: 0.9983 Cumulative_SOC_deviation: 500.1484 Fuel Consumption: 187.9889\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 70 Total reward: -5176.595733913224 Explore P: 0.1548 SOC: 0.9964 Cumulative_SOC_deviation: 498.4134 Fuel Consumption: 192.4615\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 71 Total reward: -4959.105489964336 Explore P: 0.1509 SOC: 0.9995 Cumulative_SOC_deviation: 477.9969 Fuel Consumption: 179.1366\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 72 Total reward: -5075.328793948743 Explore P: 0.1471 SOC: 1.0000 Cumulative_SOC_deviation: 488.3400 Fuel Consumption: 191.9293\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 73 Total reward: -5133.84783809324 Explore P: 0.1434 SOC: 0.9999 Cumulative_SOC_deviation: 495.7343 Fuel Consumption: 176.5048\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 74 Total reward: -4887.212403920003 Explore P: 0.1398 SOC: 0.9970 Cumulative_SOC_deviation: 470.8203 Fuel Consumption: 179.0097\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 75 Total reward: -4973.57958100883 Explore P: 0.1362 SOC: 0.9989 Cumulative_SOC_deviation: 479.7685 Fuel Consumption: 175.8945\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 76 Total reward: -5111.193978217022 Explore P: 0.1328 SOC: 1.0000 Cumulative_SOC_deviation: 491.9265 Fuel Consumption: 191.9293\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 77 Total reward: -5142.92760533331 Explore P: 0.1295 SOC: 1.0000 Cumulative_SOC_deviation: 496.3166 Fuel Consumption: 179.7616\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 78 Total reward: -5180.724559984618 Explore P: 0.1263 SOC: 0.9992 Cumulative_SOC_deviation: 500.4073 Fuel Consumption: 176.6513\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 79 Total reward: -5036.979323847533 Explore P: 0.1231 SOC: 1.0000 Cumulative_SOC_deviation: 485.5509 Fuel Consumption: 181.4705\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 80 Total reward: -5131.833250117676 Explore P: 0.1200 SOC: 0.9997 Cumulative_SOC_deviation: 494.1960 Fuel Consumption: 189.8737\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 81 Total reward: -4987.519079280492 Explore P: 0.1171 SOC: 0.9983 Cumulative_SOC_deviation: 481.1200 Fuel Consumption: 176.3193\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 82 Total reward: -5100.286531546276 Explore P: 0.1142 SOC: 1.0000 Cumulative_SOC_deviation: 491.2517 Fuel Consumption: 187.7692\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 83 Total reward: -5130.817767011763 Explore P: 0.1113 SOC: 1.0000 Cumulative_SOC_deviation: 493.2121 Fuel Consumption: 198.6967\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 84 Total reward: -5248.482762267075 Explore P: 0.1086 SOC: 1.0000 Cumulative_SOC_deviation: 504.2018 Fuel Consumption: 206.4651\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 85 Total reward: -5164.208885604741 Explore P: 0.1059 SOC: 1.0000 Cumulative_SOC_deviation: 496.8979 Fuel Consumption: 195.2300\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 86 Total reward: -5150.748541607923 Explore P: 0.1033 SOC: 1.0000 Cumulative_SOC_deviation: 495.7027 Fuel Consumption: 193.7212\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 87 Total reward: -5019.823941526023 Explore P: 0.1008 SOC: 1.0000 Cumulative_SOC_deviation: 482.6161 Fuel Consumption: 193.6626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 88 Total reward: -5024.694940113041 Explore P: 0.0983 SOC: 0.9997 Cumulative_SOC_deviation: 483.8742 Fuel Consumption: 185.9529\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 89 Total reward: -5049.500022109033 Explore P: 0.0960 SOC: 1.0000 Cumulative_SOC_deviation: 487.1340 Fuel Consumption: 178.1601\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 90 Total reward: -5062.010979837224 Explore P: 0.0936 SOC: 1.0000 Cumulative_SOC_deviation: 487.3006 Fuel Consumption: 189.0045\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 91 Total reward: -5228.1864276819315 Explore P: 0.0914 SOC: 0.9962 Cumulative_SOC_deviation: 503.7317 Fuel Consumption: 190.8697\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 92 Total reward: -4991.241864465079 Explore P: 0.0892 SOC: 1.0000 Cumulative_SOC_deviation: 480.4996 Fuel Consumption: 186.2458\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 93 Total reward: -5118.238533985321 Explore P: 0.0870 SOC: 1.0000 Cumulative_SOC_deviation: 492.7696 Fuel Consumption: 190.5426\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 94 Total reward: -5062.898790460116 Explore P: 0.0849 SOC: 1.0000 Cumulative_SOC_deviation: 488.1487 Fuel Consumption: 181.4119\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 95 Total reward: -4723.531804125195 Explore P: 0.0829 SOC: 1.0000 Cumulative_SOC_deviation: 454.9092 Fuel Consumption: 174.4394\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 96 Total reward: -5101.656979196005 Explore P: 0.0809 SOC: 1.0000 Cumulative_SOC_deviation: 491.5099 Fuel Consumption: 186.5583\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 97 Total reward: -4990.7937426142435 Explore P: 0.0790 SOC: 0.9987 Cumulative_SOC_deviation: 481.8566 Fuel Consumption: 172.2276\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 98 Total reward: -5101.931026295196 Explore P: 0.0771 SOC: 0.9861 Cumulative_SOC_deviation: 492.1203 Fuel Consumption: 180.7284\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 99 Total reward: -5128.049390025649 Explore P: 0.0753 SOC: 1.0000 Cumulative_SOC_deviation: 493.5368 Fuel Consumption: 192.6812\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 100 Total reward: -5112.189998499963 Explore P: 0.0735 SOC: 1.0000 Cumulative_SOC_deviation: 492.4631 Fuel Consumption: 187.5593\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 101 Total reward: -5209.202621442755 Explore P: 0.0718 SOC: 1.0000 Cumulative_SOC_deviation: 500.4378 Fuel Consumption: 204.8245\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 102 Total reward: -5093.463390147528 Explore P: 0.0701 SOC: 1.0000 Cumulative_SOC_deviation: 490.7867 Fuel Consumption: 185.5964\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 103 Total reward: -5043.6029438126025 Explore P: 0.0685 SOC: 1.0000 Cumulative_SOC_deviation: 486.0985 Fuel Consumption: 182.6180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 104 Total reward: -5053.875160796781 Explore P: 0.0669 SOC: 1.0000 Cumulative_SOC_deviation: 486.3093 Fuel Consumption: 190.7818\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 105 Total reward: -4914.586839176069 Explore P: 0.0654 SOC: 1.0000 Cumulative_SOC_deviation: 472.1676 Fuel Consumption: 192.9107\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 106 Total reward: -5203.110909937327 Explore P: 0.0639 SOC: 0.9979 Cumulative_SOC_deviation: 501.4321 Fuel Consumption: 188.7897\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 107 Total reward: -4974.363434594838 Explore P: 0.0624 SOC: 1.0000 Cumulative_SOC_deviation: 479.1931 Fuel Consumption: 182.4324\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 108 Total reward: -4987.709134871582 Explore P: 0.0610 SOC: 1.0000 Cumulative_SOC_deviation: 479.2577 Fuel Consumption: 195.1323\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 109 Total reward: -5220.530225893859 Explore P: 0.0596 SOC: 0.9995 Cumulative_SOC_deviation: 503.2854 Fuel Consumption: 187.6765\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 110 Total reward: -5055.934983440472 Explore P: 0.0583 SOC: 1.0000 Cumulative_SOC_deviation: 487.3005 Fuel Consumption: 182.9305\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 111 Total reward: -4991.1870214356195 Explore P: 0.0570 SOC: 1.0000 Cumulative_SOC_deviation: 480.3066 Fuel Consumption: 188.1208\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 112 Total reward: -4870.46111053935 Explore P: 0.0557 SOC: 1.0000 Cumulative_SOC_deviation: 469.6310 Fuel Consumption: 174.1514\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 113 Total reward: -5090.141300875431 Explore P: 0.0545 SOC: 1.0000 Cumulative_SOC_deviation: 491.1039 Fuel Consumption: 179.1024\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 114 Total reward: -5140.659276404344 Explore P: 0.0533 SOC: 1.0000 Cumulative_SOC_deviation: 494.9145 Fuel Consumption: 191.5143\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 115 Total reward: -5192.273894434343 Explore P: 0.0521 SOC: 1.0000 Cumulative_SOC_deviation: 499.8597 Fuel Consumption: 193.6773\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 116 Total reward: -5136.227829845822 Explore P: 0.0510 SOC: 0.9984 Cumulative_SOC_deviation: 494.6838 Fuel Consumption: 189.3903\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 117 Total reward: -4965.563282597786 Explore P: 0.0498 SOC: 1.0000 Cumulative_SOC_deviation: 478.2638 Fuel Consumption: 182.9256\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 118 Total reward: -5163.57316142123 Explore P: 0.0488 SOC: 1.0000 Cumulative_SOC_deviation: 496.0599 Fuel Consumption: 202.9740\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 119 Total reward: -4922.979942910461 Explore P: 0.0477 SOC: 1.0000 Cumulative_SOC_deviation: 473.6920 Fuel Consumption: 186.0603\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 120 Total reward: -4720.9500588411975 Explore P: 0.0467 SOC: 1.0000 Cumulative_SOC_deviation: 453.9997 Fuel Consumption: 180.9530\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 121 Total reward: -4524.540046417213 Explore P: 0.0457 SOC: 0.9979 Cumulative_SOC_deviation: 434.0257 Fuel Consumption: 184.2830\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 122 Total reward: -4613.055304780985 Explore P: 0.0447 SOC: 1.0000 Cumulative_SOC_deviation: 442.1331 Fuel Consumption: 191.7242\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 123 Total reward: -4802.628001227832 Explore P: 0.0438 SOC: 0.9987 Cumulative_SOC_deviation: 461.2847 Fuel Consumption: 189.7809\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 124 Total reward: -4722.92456097576 Explore P: 0.0429 SOC: 0.9995 Cumulative_SOC_deviation: 454.6000 Fuel Consumption: 176.9247\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 125 Total reward: -4708.48628560369 Explore P: 0.0420 SOC: 0.9995 Cumulative_SOC_deviation: 453.1332 Fuel Consumption: 177.1542\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 126 Total reward: -4727.457208060772 Explore P: 0.0411 SOC: 0.9987 Cumulative_SOC_deviation: 454.1421 Fuel Consumption: 186.0359\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 127 Total reward: -4692.811198974249 Explore P: 0.0403 SOC: 0.9985 Cumulative_SOC_deviation: 451.0218 Fuel Consumption: 182.5936\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 128 Total reward: -4612.024584294346 Explore P: 0.0395 SOC: 1.0000 Cumulative_SOC_deviation: 442.8030 Fuel Consumption: 183.9949\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 129 Total reward: -4731.991506925458 Explore P: 0.0387 SOC: 0.9995 Cumulative_SOC_deviation: 455.3133 Fuel Consumption: 178.8583\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 130 Total reward: -4902.804020522282 Explore P: 0.0379 SOC: 1.0000 Cumulative_SOC_deviation: 470.0924 Fuel Consumption: 201.8802\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 131 Total reward: -4576.93881480972 Explore P: 0.0371 SOC: 1.0000 Cumulative_SOC_deviation: 439.8574 Fuel Consumption: 178.3651\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 132 Total reward: -4671.618744012814 Explore P: 0.0364 SOC: 0.9963 Cumulative_SOC_deviation: 450.1398 Fuel Consumption: 170.2208\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 133 Total reward: -4705.169943405546 Explore P: 0.0357 SOC: 1.0000 Cumulative_SOC_deviation: 453.3206 Fuel Consumption: 171.9639\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 134 Total reward: -4803.538089410646 Explore P: 0.0350 SOC: 0.9986 Cumulative_SOC_deviation: 462.3552 Fuel Consumption: 179.9862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 135 Total reward: -4657.894024475742 Explore P: 0.0343 SOC: 0.9963 Cumulative_SOC_deviation: 448.9914 Fuel Consumption: 167.9796\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 136 Total reward: -4494.926209820926 Explore P: 0.0336 SOC: 0.9963 Cumulative_SOC_deviation: 432.0267 Fuel Consumption: 174.6592\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 137 Total reward: -4275.036845912624 Explore P: 0.0330 SOC: 0.9963 Cumulative_SOC_deviation: 409.1677 Fuel Consumption: 183.3601\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 138 Total reward: -4783.329941097326 Explore P: 0.0324 SOC: 0.9891 Cumulative_SOC_deviation: 460.6366 Fuel Consumption: 176.9638\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 139 Total reward: -4656.564650772797 Explore P: 0.0318 SOC: 0.9947 Cumulative_SOC_deviation: 448.8380 Fuel Consumption: 168.1847\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 140 Total reward: -4504.718424575953 Explore P: 0.0312 SOC: 0.9946 Cumulative_SOC_deviation: 433.2901 Fuel Consumption: 171.8174\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 141 Total reward: -4444.76640745781 Explore P: 0.0306 SOC: 0.9963 Cumulative_SOC_deviation: 429.1352 Fuel Consumption: 153.4145\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 142 Total reward: -4324.249022247593 Explore P: 0.0301 SOC: 0.9677 Cumulative_SOC_deviation: 417.4785 Fuel Consumption: 149.4644\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 143 Total reward: -4943.716200032258 Explore P: 0.0295 SOC: 0.9963 Cumulative_SOC_deviation: 476.6313 Fuel Consumption: 177.4032\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 144 Total reward: -4998.2878859672355 Explore P: 0.0290 SOC: 0.9963 Cumulative_SOC_deviation: 481.4479 Fuel Consumption: 183.8094\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 145 Total reward: -4202.7006428605055 Explore P: 0.0285 SOC: 0.9717 Cumulative_SOC_deviation: 404.6576 Fuel Consumption: 156.1244\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 146 Total reward: -973.6854113440945 Explore P: 0.0280 SOC: 0.5444 Cumulative_SOC_deviation: 86.5949 Fuel Consumption: 107.7369\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 147 Total reward: -559.5516349777572 Explore P: 0.0275 SOC: 0.7703 Cumulative_SOC_deviation: 43.4750 Fuel Consumption: 124.8019\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 148 Total reward: -3282.7903030771404 Explore P: 0.0270 SOC: 0.9357 Cumulative_SOC_deviation: 314.5303 Fuel Consumption: 137.4872\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 149 Total reward: -1304.7621319408322 Explore P: 0.0265 SOC: 0.7270 Cumulative_SOC_deviation: 118.3798 Fuel Consumption: 120.9641\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 150 Total reward: -1030.2269493520002 Explore P: 0.0261 SOC: 0.7752 Cumulative_SOC_deviation: 90.5308 Fuel Consumption: 124.9191\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 151 Total reward: -2992.4865384610443 Explore P: 0.0257 SOC: 0.9847 Cumulative_SOC_deviation: 285.0454 Fuel Consumption: 142.0330\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 152 Total reward: -4036.1893004329972 Explore P: 0.0252 SOC: 0.9779 Cumulative_SOC_deviation: 389.3487 Fuel Consumption: 142.7019\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 153 Total reward: -3918.736911480019 Explore P: 0.0248 SOC: 0.9542 Cumulative_SOC_deviation: 377.9516 Fuel Consumption: 139.2205\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 154 Total reward: -3258.357002317163 Explore P: 0.0244 SOC: 0.9925 Cumulative_SOC_deviation: 310.8712 Fuel Consumption: 149.6451\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 155 Total reward: -3763.0035288309346 Explore P: 0.0240 SOC: 0.9844 Cumulative_SOC_deviation: 361.2069 Fuel Consumption: 150.9341\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 156 Total reward: -4280.97123193245 Explore P: 0.0237 SOC: 0.9981 Cumulative_SOC_deviation: 413.2742 Fuel Consumption: 148.2291\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 157 Total reward: -4882.621935099498 Explore P: 0.0233 SOC: 0.9897 Cumulative_SOC_deviation: 472.5799 Fuel Consumption: 156.8227\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 158 Total reward: -3925.4012156122876 Explore P: 0.0229 SOC: 0.9879 Cumulative_SOC_deviation: 376.3857 Fuel Consumption: 161.5442\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 159 Total reward: -2518.4035960556007 Explore P: 0.0226 SOC: 0.9968 Cumulative_SOC_deviation: 235.7406 Fuel Consumption: 160.9974\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 160 Total reward: -2201.4666661428223 Explore P: 0.0222 SOC: 0.8896 Cumulative_SOC_deviation: 204.8755 Fuel Consumption: 152.7114\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 161 Total reward: -2212.135452126749 Explore P: 0.0219 SOC: 0.8996 Cumulative_SOC_deviation: 206.0366 Fuel Consumption: 151.7691\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 162 Total reward: -3138.3235059835365 Explore P: 0.0216 SOC: 0.8786 Cumulative_SOC_deviation: 299.5275 Fuel Consumption: 143.0486\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 163 Total reward: -4113.405587077739 Explore P: 0.0213 SOC: 0.9705 Cumulative_SOC_deviation: 396.2120 Fuel Consumption: 151.2857\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 164 Total reward: -3372.144176530204 Explore P: 0.0210 SOC: 0.9554 Cumulative_SOC_deviation: 322.1689 Fuel Consumption: 150.4556\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 165 Total reward: -4181.826548882024 Explore P: 0.0207 SOC: 0.9515 Cumulative_SOC_deviation: 403.2699 Fuel Consumption: 149.1275\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 166 Total reward: -4524.622659565028 Explore P: 0.0204 SOC: 0.9933 Cumulative_SOC_deviation: 437.8229 Fuel Consumption: 146.3932\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 167 Total reward: -4915.556960170076 Explore P: 0.0201 SOC: 0.9963 Cumulative_SOC_deviation: 474.0610 Fuel Consumption: 174.9473\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 168 Total reward: -3986.0288463428324 Explore P: 0.0198 SOC: 0.9963 Cumulative_SOC_deviation: 382.5573 Fuel Consumption: 160.4554\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 169 Total reward: -5013.104183864016 Explore P: 0.0196 SOC: 0.9595 Cumulative_SOC_deviation: 484.0686 Fuel Consumption: 172.4180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 170 Total reward: -4850.6076132332755 Explore P: 0.0193 SOC: 0.9978 Cumulative_SOC_deviation: 469.6827 Fuel Consumption: 153.7807\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 171 Total reward: -5019.1800465990445 Explore P: 0.0190 SOC: 1.0000 Cumulative_SOC_deviation: 484.3930 Fuel Consumption: 175.2500\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 172 Total reward: -4352.158883055737 Explore P: 0.0188 SOC: 0.9912 Cumulative_SOC_deviation: 418.9711 Fuel Consumption: 162.4475\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 173 Total reward: -5229.829732352956 Explore P: 0.0186 SOC: 0.9950 Cumulative_SOC_deviation: 502.6099 Fuel Consumption: 203.7308\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 174 Total reward: -4736.392141159833 Explore P: 0.0183 SOC: 0.9979 Cumulative_SOC_deviation: 456.2436 Fuel Consumption: 173.9561\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 175 Total reward: -4832.926068733097 Explore P: 0.0181 SOC: 0.9982 Cumulative_SOC_deviation: 467.9468 Fuel Consumption: 153.4585\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 176 Total reward: -4373.654918230277 Explore P: 0.0179 SOC: 0.9984 Cumulative_SOC_deviation: 421.8688 Fuel Consumption: 154.9672\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 177 Total reward: -5006.862928065571 Explore P: 0.0177 SOC: 0.9895 Cumulative_SOC_deviation: 484.3507 Fuel Consumption: 163.3557\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 178 Total reward: -5036.812483479497 Explore P: 0.0175 SOC: 0.9979 Cumulative_SOC_deviation: 485.5527 Fuel Consumption: 181.2850\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 179 Total reward: -4491.166930649571 Explore P: 0.0173 SOC: 0.9907 Cumulative_SOC_deviation: 432.5604 Fuel Consumption: 165.5627\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 180 Total reward: -4698.595632360316 Explore P: 0.0171 SOC: 0.9979 Cumulative_SOC_deviation: 451.1256 Fuel Consumption: 187.3395\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 181 Total reward: -4657.655495301813 Explore P: 0.0169 SOC: 0.9977 Cumulative_SOC_deviation: 448.6468 Fuel Consumption: 171.1876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 182 Total reward: -4996.052946718925 Explore P: 0.0167 SOC: 1.0000 Cumulative_SOC_deviation: 482.1682 Fuel Consumption: 174.3711\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 183 Total reward: -4718.432688056388 Explore P: 0.0165 SOC: 0.9979 Cumulative_SOC_deviation: 454.1654 Fuel Consumption: 176.7783\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 184 Total reward: -4140.844479273612 Explore P: 0.0163 SOC: 0.9979 Cumulative_SOC_deviation: 398.8709 Fuel Consumption: 152.1353\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 185 Total reward: -3533.0949656678704 Explore P: 0.0162 SOC: 0.8997 Cumulative_SOC_deviation: 338.6770 Fuel Consumption: 146.3249\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 186 Total reward: -4098.3210702017095 Explore P: 0.0160 SOC: 0.8954 Cumulative_SOC_deviation: 395.1557 Fuel Consumption: 146.7643\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 187 Total reward: -3610.2353918011536 Explore P: 0.0158 SOC: 0.9139 Cumulative_SOC_deviation: 346.8700 Fuel Consumption: 141.5349\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 188 Total reward: -3579.0276710345092 Explore P: 0.0157 SOC: 0.8957 Cumulative_SOC_deviation: 343.7830 Fuel Consumption: 141.1980\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 189 Total reward: -3860.0693082905022 Explore P: 0.0155 SOC: 0.9792 Cumulative_SOC_deviation: 371.5351 Fuel Consumption: 144.7185\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 190 Total reward: -4199.709978471783 Explore P: 0.0154 SOC: 0.9846 Cumulative_SOC_deviation: 405.5568 Fuel Consumption: 144.1423\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 191 Total reward: -4658.5904593110245 Explore P: 0.0152 SOC: 0.9435 Cumulative_SOC_deviation: 451.0889 Fuel Consumption: 147.7018\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 192 Total reward: -4655.7522483882685 Explore P: 0.0151 SOC: 0.9988 Cumulative_SOC_deviation: 450.7743 Fuel Consumption: 148.0094\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 193 Total reward: -4282.936349041245 Explore P: 0.0149 SOC: 0.9833 Cumulative_SOC_deviation: 413.8052 Fuel Consumption: 144.8845\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 194 Total reward: -4872.199983149077 Explore P: 0.0148 SOC: 0.9995 Cumulative_SOC_deviation: 471.9464 Fuel Consumption: 152.7358\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 195 Total reward: -4870.435320869131 Explore P: 0.0147 SOC: 0.9957 Cumulative_SOC_deviation: 472.1117 Fuel Consumption: 149.3180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 196 Total reward: -5126.633706306087 Explore P: 0.0146 SOC: 0.9995 Cumulative_SOC_deviation: 494.0764 Fuel Consumption: 185.8699\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 197 Total reward: -5084.355384932571 Explore P: 0.0144 SOC: 0.9994 Cumulative_SOC_deviation: 490.8622 Fuel Consumption: 175.7334\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 198 Total reward: -5100.144284928832 Explore P: 0.0143 SOC: 1.0000 Cumulative_SOC_deviation: 491.4855 Fuel Consumption: 185.2888\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 199 Total reward: -5055.177596941154 Explore P: 0.0142 SOC: 0.9971 Cumulative_SOC_deviation: 486.1886 Fuel Consumption: 193.2916\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 200 Total reward: -5151.154081158016 Explore P: 0.0141 SOC: 0.9995 Cumulative_SOC_deviation: 495.7067 Fuel Consumption: 194.0874\n"
     ]
    }
   ],
   "source": [
    "print(\"environment version: {}\".format(env.version)) \n",
    "\n",
    "num_trials = 3 \n",
    "results_dict = {} \n",
    "\n",
    "for trial in range(num_trials): \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    env, memory, primary_network, target_network = initialization_with_rewardFactor(10)\n",
    "    for episode in range(TOTAL_EPISODES): \n",
    "        state = env.reset() \n",
    "        avg_loss = 0 \n",
    "        total_reward = 0\n",
    "        cnt = 1 \n",
    "\n",
    "        while True:\n",
    "            action, action_continue = choose_action(state, primary_network, eps)\n",
    "            next_state, reward, done = env.step(action_continue)\n",
    "            total_reward += reward \n",
    "            if done: \n",
    "                next_state = None \n",
    "            memory.add_sample((state, action, reward, next_state))\n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                loss = train(primary_network, target_network, memory)\n",
    "                update_network(primary_network, target_network)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "            else: \n",
    "                loss = -1\n",
    "\n",
    "            avg_loss += loss \n",
    "            steps += 1 \n",
    "\n",
    "            if done: \n",
    "                if steps > DELAY_TRAINING: \n",
    "                    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "                    avg_loss /= cnt \n",
    "                    print('Episode: {}'.format(episode + 1),\n",
    "                          'Total reward: {}'.format(total_reward), \n",
    "                          'Explore P: {:.4f}'.format(eps), \n",
    "                          \"SOC: {:.4f}\".format(env.SOC), \n",
    "                         \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "                         \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "                         )\n",
    "                else: \n",
    "                    print(f\"Pre-training...Episode: {episode}\")\n",
    "                \n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_SOCs.append(env.SOC)\n",
    "                episode_FCs.append(env.fuel_consumption)\n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "            cnt += 1 \n",
    "    \n",
    "    results_dict[trial] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDQN3_1.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/replay_memory_size_effect.pkl\", \"rb\") as f: \n",
    "#     data = pickle.load(f)\n",
    "    \n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
