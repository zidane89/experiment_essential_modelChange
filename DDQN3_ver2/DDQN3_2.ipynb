{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "import scipy.io as sio\n",
    "\n",
    "from vehicle_model_variant_2 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driving_cycle_path = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "driving_cycle = sio.loadmat(driving_cycle_path)\n",
    "driving_cycle = driving_cycle[\"sch_cycle\"][:, 1]\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_nimh_6_240_panasonic_MY01_Prius.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_pm_95_145_X2.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, driving_cycle, battery_path, motor_path, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATE_SIZE = env.calculation_comp[\"state_size\"]\n",
    "STATE_SIZE = 4\n",
    "ACTION_SIZE = env.calculation_comp[\"action_size\"] \n",
    "LEARNING_RATE = 0.00025 \n",
    "\n",
    "TOTAL_EPISODES = 200\n",
    "MAX_STEPS = 50000 \n",
    "\n",
    "GAMMA = 0.95 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "TAU = 0.001 \n",
    "DELAY_TRAINING = 3000 \n",
    "EPSILON_MIN_ITER = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "target_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "\n",
    "primary_network.compile(\n",
    "    loss=\"mse\", \n",
    "    optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    ")\n",
    "\n",
    "# for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "#     t.assign(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network(primary_network, target_network): \n",
    "    for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "        t.assign(t * (1 - TAU) + p * TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory: \n",
    "    def __init__(self, max_memory): \n",
    "        self.max_memory = max_memory \n",
    "        self._samples = [] \n",
    "        \n",
    "    def add_sample(self, sample): \n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self.max_memory: \n",
    "            self._samples.pop(0)\n",
    "        \n",
    "    def sample(self, no_samples): \n",
    "        if no_samples > len(self._samples): \n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else: \n",
    "            return random.sample(self._samples, no_samples)\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)\n",
    "    \n",
    "\n",
    "# memory = Memory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, primary_network, eps):\n",
    "    j_min = state[-2]\n",
    "    j_max = state[-1]\n",
    "    \n",
    "    if random.random() < eps:\n",
    "        action = random.randint(0, ACTION_SIZE - 1) \n",
    "        action_continue = np.linspace(j_min, j_max, ACTION_SIZE)[action]\n",
    "        return action, action_continue \n",
    "    else: \n",
    "        action = np.argmax(primary_network(np.array(state).reshape(1, -1)))\n",
    "        action_continue = np.linspace(j_min, j_max, ACTION_SIZE)[action]\n",
    "        return action, action_continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(primary_network, target_network, memory): \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    states = np.array([val[0] for val in batch]) \n",
    "    actions = np.array([val[1] for val in batch])\n",
    "    rewards = np.array([val[2] for val in batch])\n",
    "    next_states = np.array([np.zeros(STATE_SIZE) if val[3] is None else val[3]  \n",
    "                            for val in batch])\n",
    "    \n",
    "    prim_qt = primary_network(states)\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "    target_q = prim_qt.numpy() \n",
    "    updates = rewards \n",
    "    valid_idxs = next_states.sum(axis=1) != 0 \n",
    "    batch_idxs = np.arange(BATCH_SIZE)\n",
    "    prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "    q_from_target = target_network(next_states)\n",
    "    updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], \n",
    "                                                        prim_action_tp1[valid_idxs]]\n",
    "    \n",
    "    target_q[batch_idxs, actions] = updates \n",
    "    loss = primary_network.train_on_batch(states, target_q)\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_with_rewardFactor(reward_factor):\n",
    "    env = Environment(cell_model, driving_cycle, battery_path, motor_path, reward_factor)\n",
    "    \n",
    "    memory = Memory(10000)\n",
    "    \n",
    "    primary_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(),  \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    target_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    primary_network.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    "    )\n",
    "    return env, memory, primary_network, target_network \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment version: 1\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 0\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 1\n",
      "WARNING:tensorflow:Layer sequential_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer sequential_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 3 Total reward: -5077.251283073908 Explore P: 0.9217 SOC: 1.0000 Cumulative_SOC_deviation: 488.7329 Fuel Consumption: 189.9225\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 4 Total reward: -4983.086564206276 Explore P: 0.8970 SOC: 1.0000 Cumulative_SOC_deviation: 479.9258 Fuel Consumption: 183.8289\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 5 Total reward: -4956.364245964427 Explore P: 0.8730 SOC: 1.0000 Cumulative_SOC_deviation: 477.6666 Fuel Consumption: 179.6981\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 6 Total reward: -4886.783212503268 Explore P: 0.8496 SOC: 1.0000 Cumulative_SOC_deviation: 470.9336 Fuel Consumption: 177.4472\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 7 Total reward: -4994.205284955041 Explore P: 0.8269 SOC: 0.9995 Cumulative_SOC_deviation: 481.1919 Fuel Consumption: 182.2859\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 8 Total reward: -4830.897119718968 Explore P: 0.8048 SOC: 1.0000 Cumulative_SOC_deviation: 465.7966 Fuel Consumption: 172.9307\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 9 Total reward: -4870.052775962806 Explore P: 0.7832 SOC: 1.0000 Cumulative_SOC_deviation: 469.1566 Fuel Consumption: 178.4872\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 10 Total reward: -4877.508261999299 Explore P: 0.7623 SOC: 1.0000 Cumulative_SOC_deviation: 469.7566 Fuel Consumption: 179.9423\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 11 Total reward: -4819.694482955691 Explore P: 0.7419 SOC: 1.0000 Cumulative_SOC_deviation: 463.7892 Fuel Consumption: 181.8026\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 12 Total reward: -4437.571543802857 Explore P: 0.7221 SOC: 0.9993 Cumulative_SOC_deviation: 426.1672 Fuel Consumption: 175.8994\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 13 Total reward: -4241.177492411506 Explore P: 0.7028 SOC: 0.9992 Cumulative_SOC_deviation: 407.4492 Fuel Consumption: 166.6857\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 14 Total reward: -3978.2666552136407 Explore P: 0.6840 SOC: 1.0000 Cumulative_SOC_deviation: 382.1102 Fuel Consumption: 157.1645\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 15 Total reward: -3719.6033747289184 Explore P: 0.6658 SOC: 0.9993 Cumulative_SOC_deviation: 355.4197 Fuel Consumption: 165.4065\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 16 Total reward: -3533.5717091917586 Explore P: 0.6480 SOC: 0.9993 Cumulative_SOC_deviation: 336.7164 Fuel Consumption: 166.4074\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 17 Total reward: -3173.694266811874 Explore P: 0.6307 SOC: 1.0000 Cumulative_SOC_deviation: 301.5934 Fuel Consumption: 157.7601\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 18 Total reward: -3261.721344217045 Explore P: 0.6139 SOC: 0.9983 Cumulative_SOC_deviation: 310.4801 Fuel Consumption: 156.9203\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 19 Total reward: -3411.600900997628 Explore P: 0.5976 SOC: 0.9987 Cumulative_SOC_deviation: 325.6717 Fuel Consumption: 154.8842\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 20 Total reward: -2088.9057205782688 Explore P: 0.5816 SOC: 0.9853 Cumulative_SOC_deviation: 193.6922 Fuel Consumption: 151.9839\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 21 Total reward: -2870.369091713814 Explore P: 0.5662 SOC: 1.0000 Cumulative_SOC_deviation: 271.2174 Fuel Consumption: 158.1947\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 22 Total reward: -2380.5836226659435 Explore P: 0.5511 SOC: 1.0000 Cumulative_SOC_deviation: 222.6070 Fuel Consumption: 154.5131\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 23 Total reward: -2358.974012156986 Explore P: 0.5364 SOC: 1.0000 Cumulative_SOC_deviation: 219.7327 Fuel Consumption: 161.6468\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 24 Total reward: -1820.8898011332892 Explore P: 0.5222 SOC: 1.0000 Cumulative_SOC_deviation: 166.7290 Fuel Consumption: 153.6001\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 25 Total reward: -1697.2377187899037 Explore P: 0.5083 SOC: 1.0000 Cumulative_SOC_deviation: 154.0464 Fuel Consumption: 156.7738\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 26 Total reward: -1404.9541267390205 Explore P: 0.4948 SOC: 1.0000 Cumulative_SOC_deviation: 124.9342 Fuel Consumption: 155.6118\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 27 Total reward: -2153.693257442675 Explore P: 0.4817 SOC: 1.0000 Cumulative_SOC_deviation: 198.6558 Fuel Consumption: 167.1349\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 28 Total reward: -1105.8042224340159 Explore P: 0.4689 SOC: 0.9259 Cumulative_SOC_deviation: 95.7375 Fuel Consumption: 148.4293\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 29 Total reward: -968.9379806466029 Explore P: 0.4565 SOC: 0.9216 Cumulative_SOC_deviation: 82.1070 Fuel Consumption: 147.8678\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 30 Total reward: -651.1252755384565 Explore P: 0.4444 SOC: 0.7648 Cumulative_SOC_deviation: 51.3990 Fuel Consumption: 137.1356\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 31 Total reward: -636.6677592396408 Explore P: 0.4326 SOC: 0.7461 Cumulative_SOC_deviation: 50.1651 Fuel Consumption: 135.0165\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 32 Total reward: -558.1210806874844 Explore P: 0.4212 SOC: 0.6978 Cumulative_SOC_deviation: 42.6820 Fuel Consumption: 131.3008\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 33 Total reward: -385.41983260791733 Explore P: 0.4100 SOC: 0.6426 Cumulative_SOC_deviation: 25.8235 Fuel Consumption: 127.1847\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 34 Total reward: -361.1379034112295 Explore P: 0.3992 SOC: 0.5997 Cumulative_SOC_deviation: 23.6004 Fuel Consumption: 125.1339\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 35 Total reward: -552.2773518194289 Explore P: 0.3887 SOC: 0.6267 Cumulative_SOC_deviation: 42.6636 Fuel Consumption: 125.6417\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 36 Total reward: -495.87417573962676 Explore P: 0.3784 SOC: 0.6144 Cumulative_SOC_deviation: 36.9964 Fuel Consumption: 125.9103\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 37 Total reward: -417.84167198336013 Explore P: 0.3684 SOC: 0.6359 Cumulative_SOC_deviation: 29.2151 Fuel Consumption: 125.6906\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 38 Total reward: -991.779820247295 Explore P: 0.3587 SOC: 0.8923 Cumulative_SOC_deviation: 84.7823 Fuel Consumption: 143.9568\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 39 Total reward: -549.8367257471677 Explore P: 0.3493 SOC: 0.5862 Cumulative_SOC_deviation: 42.8165 Fuel Consumption: 121.6721\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 40 Total reward: -472.9981431977117 Explore P: 0.3401 SOC: 0.5818 Cumulative_SOC_deviation: 35.0799 Fuel Consumption: 122.1994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 41 Total reward: -531.1191736620214 Explore P: 0.3311 SOC: 0.5800 Cumulative_SOC_deviation: 41.0096 Fuel Consumption: 121.0227\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 42 Total reward: -479.92909228514975 Explore P: 0.3224 SOC: 0.5780 Cumulative_SOC_deviation: 35.8413 Fuel Consumption: 121.5159\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 43 Total reward: -494.0037487650544 Explore P: 0.3140 SOC: 0.5820 Cumulative_SOC_deviation: 37.2903 Fuel Consumption: 121.1008\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 44 Total reward: -526.5384129282735 Explore P: 0.3057 SOC: 0.5734 Cumulative_SOC_deviation: 40.4798 Fuel Consumption: 121.7405\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 45 Total reward: -556.3512465383285 Explore P: 0.2977 SOC: 0.5919 Cumulative_SOC_deviation: 43.4811 Fuel Consumption: 121.5403\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 46 Total reward: -544.1156265346635 Explore P: 0.2899 SOC: 0.5824 Cumulative_SOC_deviation: 42.1379 Fuel Consumption: 122.7365\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 47 Total reward: -515.8485065936757 Explore P: 0.2824 SOC: 0.5704 Cumulative_SOC_deviation: 39.4098 Fuel Consumption: 121.7502\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 48 Total reward: -503.0622292453756 Explore P: 0.2750 SOC: 0.5700 Cumulative_SOC_deviation: 38.2728 Fuel Consumption: 120.3342\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 49 Total reward: -553.9917981742274 Explore P: 0.2678 SOC: 0.5776 Cumulative_SOC_deviation: 43.5298 Fuel Consumption: 118.6937\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 50 Total reward: -528.5698116132495 Explore P: 0.2608 SOC: 0.5760 Cumulative_SOC_deviation: 40.7684 Fuel Consumption: 120.8860\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 51 Total reward: -506.9321934685493 Explore P: 0.2540 SOC: 0.5808 Cumulative_SOC_deviation: 38.7086 Fuel Consumption: 119.8460\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 52 Total reward: -495.7816152465989 Explore P: 0.2474 SOC: 0.5756 Cumulative_SOC_deviation: 37.6458 Fuel Consumption: 119.3235\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 53 Total reward: -480.529981335595 Explore P: 0.2410 SOC: 0.5822 Cumulative_SOC_deviation: 36.2569 Fuel Consumption: 117.9613\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 54 Total reward: -423.4995824194726 Explore P: 0.2347 SOC: 0.5787 Cumulative_SOC_deviation: 30.5323 Fuel Consumption: 118.1761\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 55 Total reward: -394.5377163478376 Explore P: 0.2286 SOC: 0.5814 Cumulative_SOC_deviation: 27.7709 Fuel Consumption: 116.8285\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 56 Total reward: -387.34669509024883 Explore P: 0.2227 SOC: 0.5893 Cumulative_SOC_deviation: 27.1182 Fuel Consumption: 116.1644\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 57 Total reward: -371.8687024304775 Explore P: 0.2170 SOC: 0.6585 Cumulative_SOC_deviation: 25.0665 Fuel Consumption: 121.2034\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 58 Total reward: -3141.4654154787704 Explore P: 0.2114 SOC: 0.9834 Cumulative_SOC_deviation: 298.7646 Fuel Consumption: 153.8198\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 59 Total reward: -294.9396712325701 Explore P: 0.2059 SOC: 0.5911 Cumulative_SOC_deviation: 17.8707 Fuel Consumption: 116.2328\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 60 Total reward: -243.28593092156777 Explore P: 0.2006 SOC: 0.5957 Cumulative_SOC_deviation: 12.6868 Fuel Consumption: 116.4183\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 61 Total reward: -1018.0264018851168 Explore P: 0.1954 SOC: 0.6459 Cumulative_SOC_deviation: 89.6428 Fuel Consumption: 121.5989\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 62 Total reward: -299.9177058222557 Explore P: 0.1904 SOC: 0.6047 Cumulative_SOC_deviation: 18.1927 Fuel Consumption: 117.9905\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 63 Total reward: -245.32784378502535 Explore P: 0.1855 SOC: 0.6030 Cumulative_SOC_deviation: 12.8275 Fuel Consumption: 117.0531\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 64 Total reward: -249.50768788214486 Explore P: 0.1808 SOC: 0.6152 Cumulative_SOC_deviation: 13.1502 Fuel Consumption: 118.0052\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 65 Total reward: -225.69447515938285 Explore P: 0.1761 SOC: 0.6060 Cumulative_SOC_deviation: 10.8617 Fuel Consumption: 117.0775\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 66 Total reward: -203.12180233057447 Explore P: 0.1716 SOC: 0.5990 Cumulative_SOC_deviation: 8.6860 Fuel Consumption: 116.2621\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 67 Total reward: -209.5590364191558 Explore P: 0.1673 SOC: 0.5906 Cumulative_SOC_deviation: 9.5631 Fuel Consumption: 113.9281\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 68 Total reward: -199.99182191520197 Explore P: 0.1630 SOC: 0.6139 Cumulative_SOC_deviation: 8.4872 Fuel Consumption: 115.1195\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 69 Total reward: -196.6905082161793 Explore P: 0.1589 SOC: 0.6096 Cumulative_SOC_deviation: 8.0741 Fuel Consumption: 115.9496\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 70 Total reward: -184.04948577001753 Explore P: 0.1548 SOC: 0.5990 Cumulative_SOC_deviation: 6.9081 Fuel Consumption: 114.9682\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 71 Total reward: -178.52257608932115 Explore P: 0.1509 SOC: 0.6000 Cumulative_SOC_deviation: 6.2524 Fuel Consumption: 115.9984\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 72 Total reward: -188.1248672908805 Explore P: 0.1471 SOC: 0.6001 Cumulative_SOC_deviation: 7.2258 Fuel Consumption: 115.8666\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 73 Total reward: -198.27370022315176 Explore P: 0.1434 SOC: 0.5976 Cumulative_SOC_deviation: 8.2397 Fuel Consumption: 115.8763\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 74 Total reward: -188.11799858303866 Explore P: 0.1398 SOC: 0.5939 Cumulative_SOC_deviation: 7.3365 Fuel Consumption: 114.7533\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 75 Total reward: -178.30316690109052 Explore P: 0.1362 SOC: 0.6042 Cumulative_SOC_deviation: 6.2324 Fuel Consumption: 115.9789\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 76 Total reward: -177.45259594272935 Explore P: 0.1328 SOC: 0.5961 Cumulative_SOC_deviation: 6.3285 Fuel Consumption: 114.1674\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 77 Total reward: -163.87346714166284 Explore P: 0.1295 SOC: 0.6041 Cumulative_SOC_deviation: 4.6972 Fuel Consumption: 116.9017\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 78 Total reward: -173.69409882907388 Explore P: 0.1263 SOC: 0.5963 Cumulative_SOC_deviation: 6.0279 Fuel Consumption: 113.4155\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 79 Total reward: -156.28489929609083 Explore P: 0.1231 SOC: 0.6026 Cumulative_SOC_deviation: 4.3094 Fuel Consumption: 113.1909\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 80 Total reward: -169.27661406790043 Explore P: 0.1200 SOC: 0.5996 Cumulative_SOC_deviation: 5.6598 Fuel Consumption: 112.6782\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 81 Total reward: -162.481714510339 Explore P: 0.1171 SOC: 0.6011 Cumulative_SOC_deviation: 4.9257 Fuel Consumption: 113.2250\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 82 Total reward: -160.87030207380926 Explore P: 0.1142 SOC: 0.5992 Cumulative_SOC_deviation: 4.7528 Fuel Consumption: 113.3422\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 83 Total reward: -164.51224471197276 Explore P: 0.1113 SOC: 0.5971 Cumulative_SOC_deviation: 5.1727 Fuel Consumption: 112.7856\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 84 Total reward: -159.135038529865 Explore P: 0.1086 SOC: 0.6035 Cumulative_SOC_deviation: 4.5842 Fuel Consumption: 113.2934\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 85 Total reward: -160.96419212280907 Explore P: 0.1059 SOC: 0.6006 Cumulative_SOC_deviation: 4.7798 Fuel Consumption: 113.1664\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 86 Total reward: -156.01115872582074 Explore P: 0.1033 SOC: 0.5984 Cumulative_SOC_deviation: 4.2112 Fuel Consumption: 113.8988\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 87 Total reward: -153.1358101470226 Explore P: 0.1008 SOC: 0.5992 Cumulative_SOC_deviation: 4.0008 Fuel Consumption: 113.1274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 88 Total reward: -156.01282674686706 Explore P: 0.0983 SOC: 0.6017 Cumulative_SOC_deviation: 4.2788 Fuel Consumption: 113.2250\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 89 Total reward: -151.18564009645948 Explore P: 0.0960 SOC: 0.5998 Cumulative_SOC_deviation: 3.8962 Fuel Consumption: 112.2241\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 90 Total reward: -159.01145166110072 Explore P: 0.0936 SOC: 0.5994 Cumulative_SOC_deviation: 4.6807 Fuel Consumption: 112.2045\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 91 Total reward: -141.2359523992491 Explore P: 0.0914 SOC: 0.5992 Cumulative_SOC_deviation: 2.9661 Fuel Consumption: 111.5747\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 92 Total reward: -158.20255155825518 Explore P: 0.0892 SOC: 0.5984 Cumulative_SOC_deviation: 4.5671 Fuel Consumption: 112.5317\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 93 Total reward: -165.20734351421729 Explore P: 0.0870 SOC: 0.5986 Cumulative_SOC_deviation: 5.2607 Fuel Consumption: 112.6000\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 94 Total reward: -157.44945000835156 Explore P: 0.0849 SOC: 0.6001 Cumulative_SOC_deviation: 4.5201 Fuel Consumption: 112.2485\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 95 Total reward: -154.47410889861075 Explore P: 0.0829 SOC: 0.5998 Cumulative_SOC_deviation: 4.2592 Fuel Consumption: 111.8823\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 96 Total reward: -151.9772931919058 Explore P: 0.0809 SOC: 0.6062 Cumulative_SOC_deviation: 3.9011 Fuel Consumption: 112.9662\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 97 Total reward: -147.48642784936385 Explore P: 0.0790 SOC: 0.5995 Cumulative_SOC_deviation: 3.5404 Fuel Consumption: 112.0825\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 98 Total reward: -150.07524834187424 Explore P: 0.0771 SOC: 0.6000 Cumulative_SOC_deviation: 3.7944 Fuel Consumption: 112.1313\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 99 Total reward: -144.8409134821177 Explore P: 0.0753 SOC: 0.5996 Cumulative_SOC_deviation: 3.4267 Fuel Consumption: 110.5737\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 100 Total reward: -149.52616784719442 Explore P: 0.0735 SOC: 0.5988 Cumulative_SOC_deviation: 3.8528 Fuel Consumption: 110.9985\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 101 Total reward: -146.05199093916696 Explore P: 0.0718 SOC: 0.6006 Cumulative_SOC_deviation: 3.5088 Fuel Consumption: 110.9643\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 102 Total reward: -157.43717807758136 Explore P: 0.0701 SOC: 0.6011 Cumulative_SOC_deviation: 4.5223 Fuel Consumption: 112.2143\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 103 Total reward: -172.23522267702194 Explore P: 0.0685 SOC: 0.5998 Cumulative_SOC_deviation: 6.0031 Fuel Consumption: 112.2045\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 104 Total reward: -167.64943724665434 Explore P: 0.0669 SOC: 0.5996 Cumulative_SOC_deviation: 5.5186 Fuel Consumption: 112.4633\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 105 Total reward: -155.51128312896176 Explore P: 0.0654 SOC: 0.5995 Cumulative_SOC_deviation: 4.3780 Fuel Consumption: 111.7309\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 106 Total reward: -156.87559562717334 Explore P: 0.0639 SOC: 0.5991 Cumulative_SOC_deviation: 4.3856 Fuel Consumption: 113.0200\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 107 Total reward: -160.93905240130337 Explore P: 0.0624 SOC: 0.6006 Cumulative_SOC_deviation: 4.7074 Fuel Consumption: 113.8647\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 108 Total reward: -154.89763868834507 Explore P: 0.0610 SOC: 0.5995 Cumulative_SOC_deviation: 4.2703 Fuel Consumption: 112.1948\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 109 Total reward: -141.84491996854314 Explore P: 0.0596 SOC: 0.5982 Cumulative_SOC_deviation: 3.0041 Fuel Consumption: 111.8042\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 110 Total reward: -140.06620517915923 Explore P: 0.0583 SOC: 0.6011 Cumulative_SOC_deviation: 2.9034 Fuel Consumption: 111.0327\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 111 Total reward: -143.82778253787552 Explore P: 0.0570 SOC: 0.6003 Cumulative_SOC_deviation: 3.2033 Fuel Consumption: 111.7944\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 112 Total reward: -144.98810813164658 Explore P: 0.0557 SOC: 0.6009 Cumulative_SOC_deviation: 3.2891 Fuel Consumption: 112.0971\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 113 Total reward: -149.14244573049947 Explore P: 0.0545 SOC: 0.6005 Cumulative_SOC_deviation: 3.6826 Fuel Consumption: 112.3169\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 114 Total reward: -146.04904307137141 Explore P: 0.0533 SOC: 0.5991 Cumulative_SOC_deviation: 3.4230 Fuel Consumption: 111.8188\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 115 Total reward: -148.64981494055044 Explore P: 0.0521 SOC: 0.6010 Cumulative_SOC_deviation: 3.6191 Fuel Consumption: 112.4584\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 116 Total reward: -150.50838021714196 Explore P: 0.0510 SOC: 0.6012 Cumulative_SOC_deviation: 3.7991 Fuel Consumption: 112.5170\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 117 Total reward: -151.62479495759362 Explore P: 0.0498 SOC: 0.6024 Cumulative_SOC_deviation: 3.8449 Fuel Consumption: 113.1762\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 118 Total reward: -157.12988223920385 Explore P: 0.0488 SOC: 0.6011 Cumulative_SOC_deviation: 4.4471 Fuel Consumption: 112.6586\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 119 Total reward: -147.333104262617 Explore P: 0.0477 SOC: 0.6010 Cumulative_SOC_deviation: 3.4201 Fuel Consumption: 113.1323\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 120 Total reward: -148.51107709760007 Explore P: 0.0467 SOC: 0.6017 Cumulative_SOC_deviation: 3.6922 Fuel Consumption: 111.5893\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 121 Total reward: -148.73807430037274 Explore P: 0.0457 SOC: 0.5997 Cumulative_SOC_deviation: 3.8384 Fuel Consumption: 110.3540\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 122 Total reward: -146.2893824843097 Explore P: 0.0447 SOC: 0.6006 Cumulative_SOC_deviation: 3.5003 Fuel Consumption: 111.2866\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 123 Total reward: -144.4020312410867 Explore P: 0.0438 SOC: 0.6007 Cumulative_SOC_deviation: 3.1978 Fuel Consumption: 112.4243\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 124 Total reward: -150.71091905096006 Explore P: 0.0429 SOC: 0.5977 Cumulative_SOC_deviation: 3.9029 Fuel Consumption: 111.6821\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 125 Total reward: -155.2647282694108 Explore P: 0.0420 SOC: 0.5995 Cumulative_SOC_deviation: 4.3168 Fuel Consumption: 112.0971\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 126 Total reward: -147.95804747477476 Explore P: 0.0411 SOC: 0.5973 Cumulative_SOC_deviation: 3.7204 Fuel Consumption: 110.7544\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 127 Total reward: -162.4928242173354 Explore P: 0.0403 SOC: 0.6004 Cumulative_SOC_deviation: 5.1777 Fuel Consumption: 110.7153\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 128 Total reward: -158.1518353942054 Explore P: 0.0395 SOC: 0.5990 Cumulative_SOC_deviation: 4.7607 Fuel Consumption: 110.5444\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 129 Total reward: -151.88043805456405 Explore P: 0.0387 SOC: 0.6033 Cumulative_SOC_deviation: 3.9188 Fuel Consumption: 112.6928\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 130 Total reward: -153.3552644282261 Explore P: 0.0379 SOC: 0.6012 Cumulative_SOC_deviation: 4.1136 Fuel Consumption: 112.2192\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 131 Total reward: -156.16503928785642 Explore P: 0.0371 SOC: 0.5991 Cumulative_SOC_deviation: 4.6280 Fuel Consumption: 109.8853\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 132 Total reward: -158.8167008164913 Explore P: 0.0364 SOC: 0.5982 Cumulative_SOC_deviation: 4.5797 Fuel Consumption: 113.0200\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 133 Total reward: -150.3435212838449 Explore P: 0.0357 SOC: 0.5978 Cumulative_SOC_deviation: 3.8110 Fuel Consumption: 112.2338\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 134 Total reward: -152.51922073943794 Explore P: 0.0350 SOC: 0.6009 Cumulative_SOC_deviation: 3.9709 Fuel Consumption: 112.8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 135 Total reward: -155.1114878716294 Explore P: 0.0343 SOC: 0.5967 Cumulative_SOC_deviation: 4.3088 Fuel Consumption: 112.0239\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 136 Total reward: -148.81506076300272 Explore P: 0.0336 SOC: 0.5964 Cumulative_SOC_deviation: 3.7211 Fuel Consumption: 111.6040\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 137 Total reward: -156.89537643598374 Explore P: 0.0330 SOC: 0.5969 Cumulative_SOC_deviation: 4.5125 Fuel Consumption: 111.7700\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 138 Total reward: -158.51126001681473 Explore P: 0.0324 SOC: 0.5954 Cumulative_SOC_deviation: 4.6473 Fuel Consumption: 112.0385\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 139 Total reward: -163.4162816662561 Explore P: 0.0318 SOC: 0.5999 Cumulative_SOC_deviation: 5.2027 Fuel Consumption: 111.3891\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 140 Total reward: -155.0073047414013 Explore P: 0.0312 SOC: 0.5991 Cumulative_SOC_deviation: 4.3511 Fuel Consumption: 111.4966\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 141 Total reward: -151.61336110012869 Explore P: 0.0306 SOC: 0.6001 Cumulative_SOC_deviation: 4.0717 Fuel Consumption: 110.8960\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 142 Total reward: -159.62085156813768 Explore P: 0.0301 SOC: 0.5957 Cumulative_SOC_deviation: 4.8329 Fuel Consumption: 111.2915\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 143 Total reward: -167.10428790331255 Explore P: 0.0295 SOC: 0.5963 Cumulative_SOC_deviation: 5.5935 Fuel Consumption: 111.1694\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 144 Total reward: -160.05194701283264 Explore P: 0.0290 SOC: 0.5949 Cumulative_SOC_deviation: 4.8150 Fuel Consumption: 111.9018\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 145 Total reward: -148.55864669310336 Explore P: 0.0285 SOC: 0.6005 Cumulative_SOC_deviation: 3.4596 Fuel Consumption: 113.9623\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 146 Total reward: -157.63624473333124 Explore P: 0.0280 SOC: 0.6012 Cumulative_SOC_deviation: 4.4494 Fuel Consumption: 113.1420\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 147 Total reward: -159.19844097427756 Explore P: 0.0275 SOC: 0.5947 Cumulative_SOC_deviation: 4.7502 Fuel Consumption: 111.6967\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 148 Total reward: -155.84292640987903 Explore P: 0.0270 SOC: 0.5959 Cumulative_SOC_deviation: 4.2379 Fuel Consumption: 113.4643\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 149 Total reward: -159.64666832425286 Explore P: 0.0265 SOC: 0.6003 Cumulative_SOC_deviation: 4.7120 Fuel Consumption: 112.5268\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 150 Total reward: -154.42806670469537 Explore P: 0.0261 SOC: 0.5978 Cumulative_SOC_deviation: 4.2043 Fuel Consumption: 112.3852\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 151 Total reward: -163.72374083901863 Explore P: 0.0257 SOC: 0.6011 Cumulative_SOC_deviation: 5.2540 Fuel Consumption: 111.1841\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 152 Total reward: -158.15157909224558 Explore P: 0.0252 SOC: 0.6006 Cumulative_SOC_deviation: 4.7177 Fuel Consumption: 110.9741\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 153 Total reward: -165.3577339941813 Explore P: 0.0248 SOC: 0.6001 Cumulative_SOC_deviation: 5.3402 Fuel Consumption: 111.9555\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 154 Total reward: -168.21220541248482 Explore P: 0.0244 SOC: 0.5991 Cumulative_SOC_deviation: 5.6452 Fuel Consumption: 111.7602\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 155 Total reward: -167.14295295235829 Explore P: 0.0240 SOC: 0.5976 Cumulative_SOC_deviation: 5.6369 Fuel Consumption: 110.7739\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 156 Total reward: -167.23487016999871 Explore P: 0.0237 SOC: 0.6018 Cumulative_SOC_deviation: 5.4840 Fuel Consumption: 112.3950\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 157 Total reward: -152.064405105592 Explore P: 0.0233 SOC: 0.6000 Cumulative_SOC_deviation: 4.0875 Fuel Consumption: 111.1889\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 158 Total reward: -145.39495629626464 Explore P: 0.0229 SOC: 0.5947 Cumulative_SOC_deviation: 3.4587 Fuel Consumption: 110.8081\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 159 Total reward: -146.54311159030615 Explore P: 0.0226 SOC: 0.5965 Cumulative_SOC_deviation: 3.7571 Fuel Consumption: 108.9722\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 160 Total reward: -142.45239125748165 Explore P: 0.0222 SOC: 0.5963 Cumulative_SOC_deviation: 3.3422 Fuel Consumption: 109.0308\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 161 Total reward: -145.39874288116889 Explore P: 0.0219 SOC: 0.5996 Cumulative_SOC_deviation: 3.4957 Fuel Consumption: 110.4419\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 162 Total reward: -141.78587908740906 Explore P: 0.0216 SOC: 0.5981 Cumulative_SOC_deviation: 3.1329 Fuel Consumption: 110.4565\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 163 Total reward: -149.81318624950242 Explore P: 0.0213 SOC: 0.6028 Cumulative_SOC_deviation: 3.9611 Fuel Consumption: 110.2026\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 164 Total reward: -151.5849203148736 Explore P: 0.0210 SOC: 0.5978 Cumulative_SOC_deviation: 4.0088 Fuel Consumption: 111.4966\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 165 Total reward: -163.9576824769291 Explore P: 0.0207 SOC: 0.5998 Cumulative_SOC_deviation: 5.1831 Fuel Consumption: 112.1264\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 166 Total reward: -160.7208538192434 Explore P: 0.0204 SOC: 0.5996 Cumulative_SOC_deviation: 4.9962 Fuel Consumption: 110.7593\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 167 Total reward: -169.69334290573917 Explore P: 0.0201 SOC: 0.6011 Cumulative_SOC_deviation: 5.6083 Fuel Consumption: 113.6108\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 168 Total reward: -155.68056179721495 Explore P: 0.0198 SOC: 0.5953 Cumulative_SOC_deviation: 4.5820 Fuel Consumption: 109.8609\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 169 Total reward: -170.19513183869122 Explore P: 0.0196 SOC: 0.6000 Cumulative_SOC_deviation: 5.7131 Fuel Consumption: 113.0639\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 170 Total reward: -174.7213815108393 Explore P: 0.0193 SOC: 0.5998 Cumulative_SOC_deviation: 6.2756 Fuel Consumption: 111.9653\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 171 Total reward: -172.7072033987105 Explore P: 0.0190 SOC: 0.5934 Cumulative_SOC_deviation: 6.0498 Fuel Consumption: 112.2094\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 172 Total reward: -176.06491036506492 Explore P: 0.0188 SOC: 0.6006 Cumulative_SOC_deviation: 6.4173 Fuel Consumption: 111.8921\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 173 Total reward: -154.54539699270654 Explore P: 0.0186 SOC: 0.6008 Cumulative_SOC_deviation: 4.1838 Fuel Consumption: 112.7075\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 174 Total reward: -146.74355567045416 Explore P: 0.0183 SOC: 0.6009 Cumulative_SOC_deviation: 3.5052 Fuel Consumption: 111.6919\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 175 Total reward: -156.04801453571963 Explore P: 0.0181 SOC: 0.5985 Cumulative_SOC_deviation: 4.5948 Fuel Consumption: 110.1001\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 176 Total reward: -195.38915025715403 Explore P: 0.0179 SOC: 0.5974 Cumulative_SOC_deviation: 8.6378 Fuel Consumption: 109.0113\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 177 Total reward: -165.60600365029688 Explore P: 0.0177 SOC: 0.5991 Cumulative_SOC_deviation: 5.5618 Fuel Consumption: 109.9878\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 178 Total reward: -177.98083666078773 Explore P: 0.0175 SOC: 0.5923 Cumulative_SOC_deviation: 6.7539 Fuel Consumption: 110.4419\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 179 Total reward: -178.93675358828213 Explore P: 0.0173 SOC: 0.5991 Cumulative_SOC_deviation: 6.9340 Fuel Consumption: 109.5972\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 180 Total reward: -167.86781280504974 Explore P: 0.0171 SOC: 0.6008 Cumulative_SOC_deviation: 5.7636 Fuel Consumption: 110.2319\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 181 Total reward: -162.49698348706733 Explore P: 0.0169 SOC: 0.5918 Cumulative_SOC_deviation: 5.3749 Fuel Consumption: 108.7476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 182 Total reward: -176.91194701762794 Explore P: 0.0167 SOC: 0.5973 Cumulative_SOC_deviation: 6.5669 Fuel Consumption: 111.2427\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 183 Total reward: -146.6414425807239 Explore P: 0.0165 SOC: 0.5977 Cumulative_SOC_deviation: 3.6615 Fuel Consumption: 110.0269\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 184 Total reward: -159.36610354400534 Explore P: 0.0163 SOC: 0.5984 Cumulative_SOC_deviation: 5.0106 Fuel Consumption: 109.2603\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 185 Total reward: -154.0391533181259 Explore P: 0.0162 SOC: 0.6034 Cumulative_SOC_deviation: 4.2543 Fuel Consumption: 111.4966\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 186 Total reward: -142.16174229050915 Explore P: 0.0160 SOC: 0.6028 Cumulative_SOC_deviation: 3.0724 Fuel Consumption: 111.4380\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 187 Total reward: -155.17212605721585 Explore P: 0.0158 SOC: 0.6007 Cumulative_SOC_deviation: 4.3705 Fuel Consumption: 111.4673\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 188 Total reward: -153.54356525845134 Explore P: 0.0157 SOC: 0.5928 Cumulative_SOC_deviation: 4.3033 Fuel Consumption: 110.5103\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 189 Total reward: -156.66107874728309 Explore P: 0.0155 SOC: 0.6009 Cumulative_SOC_deviation: 4.4173 Fuel Consumption: 112.4877\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 190 Total reward: -158.1487555973346 Explore P: 0.0154 SOC: 0.6030 Cumulative_SOC_deviation: 4.7219 Fuel Consumption: 110.9302\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 191 Total reward: -158.8425051149345 Explore P: 0.0152 SOC: 0.5984 Cumulative_SOC_deviation: 4.7727 Fuel Consumption: 111.1157\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 192 Total reward: -175.46102438308858 Explore P: 0.0151 SOC: 0.6020 Cumulative_SOC_deviation: 6.4687 Fuel Consumption: 110.7739\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 193 Total reward: -157.36329927419712 Explore P: 0.0149 SOC: 0.6008 Cumulative_SOC_deviation: 4.5955 Fuel Consumption: 111.4087\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 194 Total reward: -159.27259521928164 Explore P: 0.0148 SOC: 0.6031 Cumulative_SOC_deviation: 4.8235 Fuel Consumption: 111.0376\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 195 Total reward: -161.1555646927176 Explore P: 0.0147 SOC: 0.6013 Cumulative_SOC_deviation: 5.0670 Fuel Consumption: 110.4858\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 196 Total reward: -166.28651809794638 Explore P: 0.0146 SOC: 0.5994 Cumulative_SOC_deviation: 5.4692 Fuel Consumption: 111.5942\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 197 Total reward: -161.590981390294 Explore P: 0.0144 SOC: 0.5987 Cumulative_SOC_deviation: 5.0153 Fuel Consumption: 111.4380\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 198 Total reward: -155.91453194857073 Explore P: 0.0143 SOC: 0.6005 Cumulative_SOC_deviation: 4.4130 Fuel Consumption: 111.7846\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 199 Total reward: -164.0753104415418 Explore P: 0.0142 SOC: 0.5983 Cumulative_SOC_deviation: 5.3755 Fuel Consumption: 110.3198\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 200 Total reward: -153.30102884860634 Explore P: 0.0141 SOC: 0.5984 Cumulative_SOC_deviation: 4.2351 Fuel Consumption: 110.9497\n"
     ]
    }
   ],
   "source": [
    "print(\"environment version: {}\".format(env.version)) \n",
    "\n",
    "num_trials = 3 \n",
    "# reward_factors = [10]\n",
    "results_dict = {} \n",
    "\n",
    "for trial in range(num_trials): \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    env, memory, primary_network, target_network = initialization_with_rewardFactor(10)\n",
    "    for episode in range(TOTAL_EPISODES): \n",
    "        state = env.reset() \n",
    "        avg_loss = 0 \n",
    "        total_reward = 0\n",
    "        cnt = 1 \n",
    "\n",
    "        while True:\n",
    "            action, action_continue = choose_action(state, primary_network, eps)\n",
    "            next_state, reward, done = env.step(action_continue)\n",
    "            total_reward += reward \n",
    "            if done: \n",
    "                next_state = None \n",
    "            memory.add_sample((state, action, reward, next_state))\n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                loss = train(primary_network, target_network, memory)\n",
    "                update_network(primary_network, target_network)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "            else: \n",
    "                loss = -1\n",
    "\n",
    "            avg_loss += loss \n",
    "            steps += 1 \n",
    "\n",
    "            if done: \n",
    "                if steps > DELAY_TRAINING: \n",
    "                    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "                    avg_loss /= cnt \n",
    "                    print('Episode: {}'.format(episode + 1),\n",
    "                          'Total reward: {}'.format(total_reward), \n",
    "                          'Explore P: {:.4f}'.format(eps), \n",
    "                          \"SOC: {:.4f}\".format(env.SOC), \n",
    "                         \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "                         \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "                         )\n",
    "                else: \n",
    "                    print(f\"Pre-training...Episode: {episode}\")\n",
    "                \n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_SOCs.append(env.SOC)\n",
    "                episode_FCs.append(env.fuel_consumption)\n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "            cnt += 1 \n",
    "    \n",
    "    results_dict[trial] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDQN3_2.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/replay_memory_size_effect.pkl\", \"rb\") as f: \n",
    "#     data = pickle.load(f)\n",
    "    \n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
